{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":10306164,"sourceType":"datasetVersion","datasetId":6379741},{"sourceId":10318641,"sourceType":"datasetVersion","datasetId":6388407}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport lightgbm as lgb\nimport joblib\n\nimport os\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nimport kaggle_evaluation.jane_street_inference_server","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:33.420830Z","iopub.execute_input":"2024-12-29T11:20:33.421449Z","iopub.status.idle":"2024-12-29T11:20:35.637061Z","shell.execute_reply.started":"2024-12-29T11:20:33.421417Z","shell.execute_reply":"2024-12-29T11:20:35.636389Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def load_data(date_id_range=None, time_id_range=None, columns=None, return_type='pl'):\n    data_dir = '/kaggle/input/jane-street-real-time-market-data-forecasting'\n    data = pl.scan_parquet(f'{data_dir}/train.parquet')\n    \n    if date_id_range is not None:\n        start_date, end_date = date_id_range\n        data = data.filter((pl.col(\"date_id\") >= start_date) & (pl.col(\"date_id\") <= end_date))\n    \n    if time_id_range is not None:\n        start_time, end_time = time_id_range\n        data = data.filter((pl.col(\"time_id\") >= start_time) & (pl.col(\"time_id\") <= end_time))\n    \n    if columns is not None:\n        data = data.select(columns)\n\n    if return_type == 'pd':\n        return data.collect().to_pandas()\n    else:\n        return data.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:35.638240Z","iopub.execute_input":"2024-12-29T11:20:35.638691Z","iopub.status.idle":"2024-12-29T11:20:35.644606Z","shell.execute_reply.started":"2024-12-29T11:20:35.638663Z","shell.execute_reply":"2024-12-29T11:20:35.643787Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def calculate_r2(y_true, y_pred, weights):\n    numerator = np.sum(weights * (y_true - y_pred) ** 2)\n    denominator = np.sum(weights * (y_true ** 2))\n    r2_score = 1 - (numerator / denominator)\n    return r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:35.714404Z","iopub.execute_input":"2024-12-29T11:20:35.714893Z","iopub.status.idle":"2024-12-29T11:20:35.718682Z","shell.execute_reply.started":"2024-12-29T11:20:35.714866Z","shell.execute_reply":"2024-12-29T11:20:35.717862Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# def calculate_r2(y_true, y_pred, weights):\n#     # pandas DataFrame을 numpy array로 변환\n#     y_true = y_true.values.flatten()  # 또는 y_true.values.ravel()\n#     y_pred = np.array(y_pred).flatten()  # 예측값도 1차원 배열로 변환\n#     weights = np.array(weights).flatten()\n    \n#     numerator = np.sum(weights * (y_true - y_pred) ** 2)\n#     denominator = np.sum(weights * (y_true ** 2))\n#     r2_score = 1 - (numerator / denominator)\n#     return r2_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:36.015210Z","iopub.execute_input":"2024-12-29T11:20:36.015478Z","iopub.status.idle":"2024-12-29T11:20:36.019444Z","shell.execute_reply.started":"2024-12-29T11:20:36.015452Z","shell.execute_reply":"2024-12-29T11:20:36.018401Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"TARGET = 'responder_6'\nFEAT_COLS = [f\"feature_{i:02d}\" for i in range(79)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:37.481900Z","iopub.execute_input":"2024-12-29T11:20:37.482567Z","iopub.status.idle":"2024-12-29T11:20:37.486562Z","shell.execute_reply.started":"2024-12-29T11:20:37.482527Z","shell.execute_reply":"2024-12-29T11:20:37.485581Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\ndef create_catboost_features_kfold(total_days=1699, n_splits=5, cat_features=None):\n    max_valid_days = 1200\n    valid_days = min(total_days, max_valid_days)\n    valid_start = 1699 - valid_days\n    \n    fold_size = valid_days // n_splits\n    folds = [(valid_start + i * fold_size, valid_start + (i + 1) * fold_size - 1) for i in range(n_splits)]\n    \n    all_data_with_preds = None\n    catboost_models = []\n    \n    for fold_idx in range(n_splits):\n        valid_range = folds[fold_idx]\n        train_ranges = [folds[i] for i in range(n_splits) if i != fold_idx]\n        print(f'Fold {fold_idx}: Creating CatBoost predictions')\n        \n        # 검증 데이터 로드\n        valid_data = load_data(date_id_range=valid_range,\n                             #columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n                               columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS + [TARGET],\n                             return_type='pl')\n        \n        # 학습 데이터 로드\n        train_data = None\n        for train_range in train_ranges:\n            partial_train_data = load_data(date_id_range=train_range,\n                                         # columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n                                           columns=[\"date_id\", \"symbol_id\", \"weight\"] + FEAT_COLS + [TARGET],\n                                         return_type='pl')\n            if train_data is None:\n                train_data = partial_train_data\n            else:\n                train_data = train_data.vstack(partial_train_data)\n        \n        # CatBoost 모델 학습\n        catboost_model = CatBoostRegressor(\n            loss_function='RMSE',\n            eval_metric='RMSE',\n            iterations=1000,\n            learning_rate=0.03,\n            early_stopping_rounds=50,\n            verbose=100,\n            cat_features=cat_features,\n            task_type='GPU'\n        )\n        \n        # Polars to pandas conversion for CatBoost\n        train_df = train_data.to_pandas()\n        valid_df = valid_data.to_pandas()\n\n        print(f\"Using only categorical features: {cat_features}\")\n        print(f\"Train shape with only cat features: {train_df[cat_features].shape}\")\n        print(f\"Valid shape with only cat features: {valid_df[cat_features].shape}\")\n        \n        catboost_model.fit(\n            #train_df[cat_features],  # FEAT_COLS 대신 cat_features만 사용\n            #train_df[FEAT_COLS],\n            train_df[FEAT_COLS+['symbol_id']],\n            train_df[TARGET],\n            # eval_set=(valid_df[cat_features], valid_df[TARGET]),\n            # eval_set=(valid_df[FEAT_COLS], valid_df[TARGET]),\n            eval_set=(valid_df[FEAT_COLS+['symbol_id']], valid_df[TARGET]),\n            sample_weight=train_df['weight']\n        )\n        \n        # 예측값 생성\n        # valid_df['catboost_pred'] = catboost_model.predict(valid_df[cat_features])\n        # valid_df['catboost_pred'] = catboost_model.predict(valid_df[FEAT_COLS])\n        valid_df['catboost_pred'] = catboost_model.predict(valid_df[FEAT_COLS+['symbol_id']])\n\n        r2_score = calculate_r2(valid_df[TARGET], valid_df['catboost_pred'], valid_df['weight'])\n        print(f\"Catboost Fold {fold_idx} validation R2 score: {r2_score}\")\n\n        # symbol_id 컬럼 제거\n        if 'symbol_id' in valid_df.columns:\n            valid_df = valid_df.drop(columns=['symbol_id'])\n\n        # 결과 저장\n        if all_data_with_preds is None:\n            all_data_with_preds = valid_df\n        else:\n            all_data_with_preds = pd.concat([all_data_with_preds, valid_df])\n\n        # print(f\"all_data_with_preds shape: {all_data_with_preds.shape}\")\n        # print(f\"all_data_with_preds head: {all_data_with_preds.head()}\")\n        \n        catboost_models.append(catboost_model)\n    \n    return all_data_with_preds, catboost_models\n   \n\ndef train_lgb_kfold_with_catboost(total_days=1699, n_splits=5, save_model=True, save_path='models2/', cat_features=None):\n    if save_model and not os.path.exists(save_path):\n        os.makedirs(save_path)\n    \n    # CatBoost 예측값 생성\n    print(\"Creating CatBoost features...\")\n    data_with_catboost, catboost_models = create_catboost_features_kfold(\n        total_days=total_days,\n        n_splits=n_splits,\n        cat_features=CAT_FEATURES  # 카테고리컬 피처 리스트 필요\n    )\n    \n    # CatBoost 예측값을 포함한 새로운 피처 리스트\n    FEAT_COLS_WITH_CATBOOST = FEAT_COLS + ['catboost_pred']\n    \n    max_valid_days = 1200\n    valid_days = min(total_days, max_valid_days)\n    valid_start = 1699 - valid_days\n    \n    fold_size = valid_days // n_splits\n    folds = [(valid_start + i * fold_size, valid_start + (i + 1) * fold_size - 1) for i in range(n_splits)]\n    \n    cv_scores = []\n    lgb_models = []\n    \n    for fold_idx in range(n_splits):\n        valid_range = folds[fold_idx]\n        train_ranges = [folds[i] for i in range(n_splits) if i != fold_idx]\n        print(f'Fold {fold_idx}: Training LightGBM')\n        \n        # 폴드별 데이터 분할\n        valid_mask = (data_with_catboost['date_id'] >= valid_range[0]) & (data_with_catboost['date_id'] <= valid_range[1])\n        valid_data = data_with_catboost[valid_mask]\n        train_data = data_with_catboost[~valid_mask]\n        \n        # LightGBM 데이터셋 생성\n        train_ds = lgb.Dataset(train_data[FEAT_COLS_WITH_CATBOOST],\n                             label=train_data[TARGET],\n                             weight=train_data['weight'])\n        valid_ds = lgb.Dataset(valid_data[FEAT_COLS_WITH_CATBOOST],\n                             label=valid_data[TARGET],\n                             weight=valid_data['weight'],\n                             reference=train_ds)\n        \n        # LightGBM 파라미터\n        LGB_PARAMS = {\n            'objective': 'regression_l2',\n            'metric': 'rmse',\n            'learning_rate': 0.05,\n            'num_leaves': 31,\n            'max_depth': -1,\n            'random_state': 42,\n            'device': 'gpu',\n        }\n        \n        # 콜백 함수\n        callbacks = [\n            lgb.early_stopping(100),\n            lgb.log_evaluation(period=50)\n        ]\n        \n        # 모델 학습\n        model = lgb.train(\n            LGB_PARAMS,\n            train_ds,\n            num_boost_round=1000,\n            valid_sets=[train_ds, valid_ds],\n            valid_names=['train', 'valid'],\n            callbacks=callbacks\n        )\n        \n        lgb_models.append(model)\n        \n        # R2 점수 계산\n        y_valid_pred = model.predict(valid_data[FEAT_COLS_WITH_CATBOOST])\n        r2_score = calculate_r2(valid_data[TARGET], y_valid_pred, valid_data['weight'])\n        print(f\"Fold {fold_idx} validation R2 score: {r2_score}\")\n        \n        cv_scores.append(r2_score)\n    \n    # 모델 저장\n    if save_model:\n        joblib.dump({\n            'lgb_models': lgb_models,\n            'catboost_models': catboost_models\n        }, os.path.join(save_path, \"stacking_models.pkl\"))\n        print(\"Saved all models to stacking_models.pkl\")\n    \n    print(f\"Cross-validation R2 scores: {cv_scores}\")\n    print(f\"Mean R2 score: {np.mean(cv_scores)}, Std: {np.std(cv_scores)}\")\n    \n    return lgb_models, catboost_models, np.mean(cv_scores), np.std(cv_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:37.821854Z","iopub.execute_input":"2024-12-29T11:20:37.822097Z","iopub.status.idle":"2024-12-29T11:20:37.967285Z","shell.execute_reply.started":"2024-12-29T11:20:37.822074Z","shell.execute_reply":"2024-12-29T11:20:37.966647Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"'''\n# lgb with k-fold cross-validation\ndef train_lgb_kfold_single(total_days=1699, n_splits=5, save_model=True, save_path='models2/'):\n    if save_model and not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    # 여기 skip은 왜 있는건지 잘 모르겠음.\n    # # Number of dates to skip from the beginning of the dataset\n    # skip_dates = 500  # 跳过前500天\n    \n    max_valid_days = 1200  # 最多使用后1200天进行交叉验证\n    valid_days = min(total_days, max_valid_days)  # 实际用于交叉验证的天数\n    valid_start = 1699 - valid_days  # 计算交叉验证的起始日期（倒数）\n    \n    fold_size = valid_days // n_splits\n    folds = [(valid_start + i * fold_size, valid_start + (i + 1) * fold_size - 1) for i in range(n_splits)]\n    \n    cv_scores = []\n    model_group = []\n    \n    for fold_idx in range(n_splits):\n        valid_range = folds[fold_idx]\n        train_ranges = [folds[i] for i in range(n_splits) if i != fold_idx]\n        print(f'Fold {fold_idx}: validation range {valid_range}, train parts: {train_ranges}')\n\n        # load valid data\n        valid_data = load_data(date_id_range=valid_range, \n                               columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET], \n                               return_type='pl')\n        valid_weight = valid_data['weight'].to_pandas()\n\n        # load train data\n        train_data = None\n        for train_range in train_ranges:\n            partial_train_data = load_data(date_id_range=train_range,\n                                           columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET], \n                                           return_type='pl')\n            if train_data is None:\n                train_data = partial_train_data\n            else:\n                train_data = train_data.vstack(partial_train_data)\n\n        train_weight = train_data['weight'].to_pandas()\n\n        # build LightGBM dataset\n        train_ds = lgb.Dataset(train_data.select(FEAT_COLS + ['weight']).to_pandas(), \n                               label=train_data[TARGET].to_pandas(), weight=train_weight)\n        valid_ds = lgb.Dataset(valid_data.select(FEAT_COLS + ['weight']).to_pandas(), \n                               label=valid_data[TARGET].to_pandas(), weight=valid_weight, reference=train_ds)\n\n        # LightGBM parameters\n        LGB_PARAMS = {\n            'objective': 'regression_l2',\n            'metric': 'rmse',\n            'learning_rate': 0.05,\n            'num_leaves': 31,\n            'max_depth': -1,\n            'random_state': 42,\n            'device': 'gpu',\n        }\n\n        # callback functions\n        early_stopping_callback = lgb.early_stopping(100)\n        verbose_eval_callback = lgb.log_evaluation(period=50)\n\n        # train model\n        model = lgb.train(\n            LGB_PARAMS,\n            train_ds,\n            num_boost_round=1000,\n            valid_sets=[train_ds, valid_ds],\n            valid_names=['train', 'valid'],\n            callbacks=[early_stopping_callback, verbose_eval_callback],\n        )\n\n        # save model\n        model_group.append(model)\n        \n        # predict on valid set and compute R2\n        y_valid_pred = model.predict(valid_data.select(FEAT_COLS + ['weight']).to_pandas())\n        r2_score = calculate_r2(valid_data[TARGET].to_pandas(), y_valid_pred, valid_weight)\n        print(f\"Fold {fold_idx} validation R2 score: {r2_score}\")\n\n        cv_scores.append(r2_score)\n\n    # Model fusion: The output of all models is averaged\n    print(f\"Total trained models: {len(model_group)}\")\n    final_model = model_group[1]  # The structure of the first model is used\n    print(\"Averaging models...\")\n    average_predictions = lambda data: average_models(model_group, data)\n    print(\"Done.\")\n    # Save the entire model group\n    if save_model:\n        joblib.dump(final_model, \"lgb_model.pkl\")\n        print(\"Saved the final merged model to lgb_model.pkl\")\n        \n    print(f\"Cross-validation R2 scores: {cv_scores}\")\n    print(f\"Mean R2 score: {np.mean(cv_scores)}, Std: {np.std(cv_scores)}\")\n\n    return model, np.mean(cv_scores), np.std(cv_scores)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:41.029325Z","iopub.execute_input":"2024-12-29T11:20:41.029847Z","iopub.status.idle":"2024-12-29T11:20:41.043163Z","shell.execute_reply.started":"2024-12-29T11:20:41.029791Z","shell.execute_reply":"2024-12-29T11:20:41.042075Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'\\n# lgb with k-fold cross-validation\\ndef train_lgb_kfold_single(total_days=1699, n_splits=5, save_model=True, save_path=\\'models2/\\'):\\n    if save_model and not os.path.exists(save_path):\\n        os.makedirs(save_path)\\n\\n    # 여기 skip은 왜 있는건지 잘 모르겠음.\\n    # # Number of dates to skip from the beginning of the dataset\\n    # skip_dates = 500  # 跳过前500天\\n    \\n    max_valid_days = 1200  # 最多使用后1200天进行交叉验证\\n    valid_days = min(total_days, max_valid_days)  # 实际用于交叉验证的天数\\n    valid_start = 1699 - valid_days  # 计算交叉验证的起始日期（倒数）\\n    \\n    fold_size = valid_days // n_splits\\n    folds = [(valid_start + i * fold_size, valid_start + (i + 1) * fold_size - 1) for i in range(n_splits)]\\n    \\n    cv_scores = []\\n    model_group = []\\n    \\n    for fold_idx in range(n_splits):\\n        valid_range = folds[fold_idx]\\n        train_ranges = [folds[i] for i in range(n_splits) if i != fold_idx]\\n        print(f\\'Fold {fold_idx}: validation range {valid_range}, train parts: {train_ranges}\\')\\n\\n        # load valid data\\n        valid_data = load_data(date_id_range=valid_range, \\n                               columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET], \\n                               return_type=\\'pl\\')\\n        valid_weight = valid_data[\\'weight\\'].to_pandas()\\n\\n        # load train data\\n        train_data = None\\n        for train_range in train_ranges:\\n            partial_train_data = load_data(date_id_range=train_range,\\n                                           columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET], \\n                                           return_type=\\'pl\\')\\n            if train_data is None:\\n                train_data = partial_train_data\\n            else:\\n                train_data = train_data.vstack(partial_train_data)\\n\\n        train_weight = train_data[\\'weight\\'].to_pandas()\\n\\n        # build LightGBM dataset\\n        train_ds = lgb.Dataset(train_data.select(FEAT_COLS + [\\'weight\\']).to_pandas(), \\n                               label=train_data[TARGET].to_pandas(), weight=train_weight)\\n        valid_ds = lgb.Dataset(valid_data.select(FEAT_COLS + [\\'weight\\']).to_pandas(), \\n                               label=valid_data[TARGET].to_pandas(), weight=valid_weight, reference=train_ds)\\n\\n        # LightGBM parameters\\n        LGB_PARAMS = {\\n            \\'objective\\': \\'regression_l2\\',\\n            \\'metric\\': \\'rmse\\',\\n            \\'learning_rate\\': 0.05,\\n            \\'num_leaves\\': 31,\\n            \\'max_depth\\': -1,\\n            \\'random_state\\': 42,\\n            \\'device\\': \\'gpu\\',\\n        }\\n\\n        # callback functions\\n        early_stopping_callback = lgb.early_stopping(100)\\n        verbose_eval_callback = lgb.log_evaluation(period=50)\\n\\n        # train model\\n        model = lgb.train(\\n            LGB_PARAMS,\\n            train_ds,\\n            num_boost_round=1000,\\n            valid_sets=[train_ds, valid_ds],\\n            valid_names=[\\'train\\', \\'valid\\'],\\n            callbacks=[early_stopping_callback, verbose_eval_callback],\\n        )\\n\\n        # save model\\n        model_group.append(model)\\n        \\n        # predict on valid set and compute R2\\n        y_valid_pred = model.predict(valid_data.select(FEAT_COLS + [\\'weight\\']).to_pandas())\\n        r2_score = calculate_r2(valid_data[TARGET].to_pandas(), y_valid_pred, valid_weight)\\n        print(f\"Fold {fold_idx} validation R2 score: {r2_score}\")\\n\\n        cv_scores.append(r2_score)\\n\\n    # Model fusion: The output of all models is averaged\\n    print(f\"Total trained models: {len(model_group)}\")\\n    final_model = model_group[1]  # The structure of the first model is used\\n    print(\"Averaging models...\")\\n    average_predictions = lambda data: average_models(model_group, data)\\n    print(\"Done.\")\\n    # Save the entire model group\\n    if save_model:\\n        joblib.dump(final_model, \"lgb_model.pkl\")\\n        print(\"Saved the final merged model to lgb_model.pkl\")\\n        \\n    print(f\"Cross-validation R2 scores: {cv_scores}\")\\n    print(f\"Mean R2 score: {np.mean(cv_scores)}, Std: {np.std(cv_scores)}\")\\n\\n    return model, np.mean(cv_scores), np.std(cv_scores)\\n'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# def train_lgb_holdout_single(total_days=1699, valid_days=120, save_model=True, save_path='models/'):\n\n#     if save_model and not os.path.exists(save_path):\n#         os.makedirs(save_path)\n\n#     skip_date = 0\n#     valid_start = total_days - valid_days  # Start date for validation data\n\n#     print(f'Validation range: {valid_start} to {total_days - 1}')\n\n#     # Load validation data\n#     valid_data = load_data(\n#         date_id_range=(valid_start, total_days - 1),\n#         columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n#         return_type='pl'\n#     )\n#     # valid_weight = valid_data.select(\"weight\").to_pandas()\n#     valid_weight = valid_data.select(\"weight\").to_pandas()[\"weight\"]\n\n#     # Load training data\n#     train_data = load_data(\n#         date_id_range=(skip_date, valid_start - 1),  # Use all available data up to the validation start\n#         columns=[\"date_id\", \"weight\"] + FEAT_COLS + [TARGET],\n#         return_type='pl'\n#     )\n    \n#     # train_weight = train_data.select(\"weight\").to_pandas()\n#     train_weight = train_data.select(\"weight\").to_pandas()[\"weight\"]\n\n#     # Build LightGBM dataset\n#     train_ds = lgb.Dataset(\n#         train_data.select(FEAT_COLS + ['weight']).to_pandas(),\n#         label=train_data.select(TARGET).to_pandas()[TARGET], \n#         weight=train_weight\n#     )\n#     # train_ds = lgb.Dataset(\n#     #     train_data.select(FEAT_COLS + ['weight']).to_pandas(),\n#     #     label=train_data.select(TARGET).to_pandas(), \n#     #     weight=train_weight\n#     # )\n#     valid_ds = lgb.Dataset(\n#         valid_data.select(FEAT_COLS + ['weight']).to_pandas(),\n#         label=valid_data.select(TARGET).to_pandas()[TARGET], \n#         weight=valid_weight, \n#         reference=train_ds\n#     )\n#     # valid_ds = lgb.Dataset(\n#     #     valid_data.select(FEAT_COLS + ['weight']).to_pandas(),\n#     #     label=valid_data.select(TARGET).to_pandas(), \n#     #     weight=valid_weight, \n#     #     reference=train_ds\n#     # )\n\n#     # LightGBM parameters\n#     LGB_PARAMS = {\n#         'objective': 'regression_l2',\n#         'metric': 'rmse',\n#         'learning_rate': 0.05,\n#         'num_leaves': 31,\n#         'max_depth': -1,\n#         'random_state': 42,\n#         'device': 'gpu',\n#     }\n\n#     # Callback functions\n#     early_stopping_callback = lgb.early_stopping(100)\n#     verbose_eval_callback = lgb.log_evaluation(period=50)\n\n#     # Train model\n#     model = lgb.train(\n#         LGB_PARAMS,\n#         train_ds,\n#         num_boost_round=1000,\n#         valid_sets=[train_ds, valid_ds],\n#         valid_names=['train', 'valid'],\n#         callbacks=[early_stopping_callback, verbose_eval_callback],\n#     )\n\n#     # Predict on validation set and compute R2\n#     y_valid_pred = model.predict(valid_data.select(FEAT_COLS + ['weight']).to_pandas())\n#     r2_score = calculate_r2(valid_data.select(TARGET).to_pandas(), y_valid_pred, valid_weight)\n#     print(f\"Validation R2 score: {r2_score}\")\n\n#     # Save the model\n#     if save_model:\n#         model_path = os.path.join(save_path, \"lgb_model.pkl\")\n#         joblib.dump(model, model_path)\n#         print(f\"Saved the model to {model_path}\")\n\n#     return model, r2_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:43.121568Z","iopub.execute_input":"2024-12-29T11:20:43.121946Z","iopub.status.idle":"2024-12-29T11:20:43.127786Z","shell.execute_reply.started":"2024-12-29T11:20:43.121918Z","shell.execute_reply":"2024-12-29T11:20:43.126867Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### First Version: Model Training","metadata":{}},{"cell_type":"code","source":"# total_days = 1699 # Total num of diff date_id = 1699\n# valid_days = 120\n# lgb_models, _ = train_lgb_holdout_single(total_days=total_days,valid_days=valid_days)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:45.182974Z","iopub.execute_input":"2024-12-29T11:20:45.183299Z","iopub.status.idle":"2024-12-29T11:20:45.187217Z","shell.execute_reply.started":"2024-12-29T11:20:45.183272Z","shell.execute_reply":"2024-12-29T11:20:45.186320Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# total_days = 500 # Total num of diff date_id = 1699\n# lgb_models, _, _ = train_lgb_kfold_single(total_days=total_days)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:46.603464Z","iopub.execute_input":"2024-12-29T11:20:46.604020Z","iopub.status.idle":"2024-12-29T11:20:46.607547Z","shell.execute_reply.started":"2024-12-29T11:20:46.603987Z","shell.execute_reply":"2024-12-29T11:20:46.606698Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# CAT_FEATURES 리스트 정의 필요\n# CAT_FEATURES = ['feature_09','feature_10','feature_11']  # 카테고리형 변수명 리스트\nCAT_FEATURES = ['feature_09','feature_10','feature_11', 'symbol_id']\n\n# 학습 실행\nlgb_models, catboost_models, mean_r2, std_r2 = train_lgb_kfold_with_catboost(\n    total_days=30,\n    cat_features=CAT_FEATURES\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:20:47.361607Z","iopub.execute_input":"2024-12-29T11:20:47.362005Z","iopub.status.idle":"2024-12-29T11:23:04.622589Z","shell.execute_reply.started":"2024-12-29T11:20:47.361977Z","shell.execute_reply":"2024-12-29T11:23:04.621678Z"}},"outputs":[{"name":"stdout","text":"Creating CatBoost features...\nFold 0: Creating CatBoost predictions\nUsing only categorical features: ['feature_09', 'feature_10', 'feature_11', 'symbol_id']\nTrain shape with only cat features: (900240, 4)\nValid shape with only cat features: (218768, 4)\n0:\tlearn: 0.7171150\ttest: 0.8072837\tbest: 0.8072837 (0)\ttotal: 133ms\tremaining: 2m 13s\n100:\tlearn: 0.7094131\ttest: 0.8053134\tbest: 0.8053052 (98)\ttotal: 4.87s\tremaining: 43.4s\n200:\tlearn: 0.7044933\ttest: 0.8046426\tbest: 0.8046426 (200)\ttotal: 9.3s\tremaining: 37s\n300:\tlearn: 0.7001660\ttest: 0.8043937\tbest: 0.8043818 (283)\ttotal: 13.8s\tremaining: 32.1s\nbestTest = 0.8043091426\nbestIteration = 313\nShrink model to first 314 iterations.\nCatboost Fold 0 validation R2 score: 0.007226412017324213\nFold 1: Creating CatBoost predictions\nUsing only categorical features: ['feature_09', 'feature_10', 'feature_11', 'symbol_id']\nTrain shape with only cat features: (895400, 4)\nValid shape with only cat features: (223608, 4)\n0:\tlearn: 0.7372981\ttest: 0.7262951\tbest: 0.7262951 (0)\ttotal: 60.6ms\tremaining: 1m\n100:\tlearn: 0.7292597\ttest: 0.7248718\tbest: 0.7248446 (97)\ttotal: 4.68s\tremaining: 41.7s\nbestTest = 0.7248046744\nbestIteration = 118\nShrink model to first 119 iterations.\nCatboost Fold 1 validation R2 score: 0.004184308979299223\nFold 2: Creating CatBoost predictions\nUsing only categorical features: ['feature_09', 'feature_10', 'feature_11', 'symbol_id']\nTrain shape with only cat features: (894432, 4)\nValid shape with only cat features: (224576, 4)\n0:\tlearn: 0.7210785\ttest: 0.7662977\tbest: 0.7662977 (0)\ttotal: 58.4ms\tremaining: 58.3s\n100:\tlearn: 0.7133089\ttest: 0.7644807\tbest: 0.7644038 (93)\ttotal: 4.66s\tremaining: 41.5s\nbestTest = 0.7644038128\nbestIteration = 93\nShrink model to first 94 iterations.\nCatboost Fold 2 validation R2 score: 0.00393692072771179\nFold 3: Creating CatBoost predictions\nUsing only categorical features: ['feature_09', 'feature_10', 'feature_11', 'symbol_id']\nTrain shape with only cat features: (892496, 4)\nValid shape with only cat features: (226512, 4)\n0:\tlearn: 0.7469192\ttest: 0.6857784\tbest: 0.6857784 (0)\ttotal: 56.7ms\tremaining: 56.7s\n100:\tlearn: 0.7389678\ttest: 0.6838193\tbest: 0.6838165 (99)\ttotal: 4.6s\tremaining: 40.9s\nbestTest = 0.6836264335\nbestIteration = 131\nShrink model to first 132 iterations.\nCatboost Fold 3 validation R2 score: 0.006630295488322835\nFold 4: Creating CatBoost predictions\nUsing only categorical features: ['feature_09', 'feature_10', 'feature_11', 'symbol_id']\nTrain shape with only cat features: (893464, 4)\nValid shape with only cat features: (225544, 4)\n0:\tlearn: 0.7268791\ttest: 0.7784442\tbest: 0.7784442 (0)\ttotal: 58.2ms\tremaining: 58.1s\n100:\tlearn: 0.7193394\ttest: 0.7763195\tbest: 0.7762706 (94)\ttotal: 4.68s\tremaining: 41.6s\nbestTest = 0.7761587244\nbestIteration = 127\nShrink model to first 128 iterations.\nCatboost Fold 4 validation R2 score: 0.006677257677948023\nFold 0: Training LightGBM\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 19465\n[LightGBM] [Info] Number of data points in the train set: 900240, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (68.68 MB) transferred to GPU in 0.087664 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.005068\nTraining until validation scores don't improve for 100 rounds\n[50]\ttrain's rmse: 0.702838\tvalid's rmse: 0.780189\n[100]\ttrain's rmse: 0.693456\tvalid's rmse: 0.781541\nEarly stopping, best iteration is:\n[33]\ttrain's rmse: 0.706841\tvalid's rmse: 0.778853\nFold 0 validation R2 score: 0.004633637729981821\nFold 1: Training LightGBM\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 19465\n[LightGBM] [Info] Number of data points in the train set: 895400, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (68.31 MB) transferred to GPU in 0.055498 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.002531\nTraining until validation scores don't improve for 100 rounds\n[50]\ttrain's rmse: 0.722269\tvalid's rmse: 0.695783\n[100]\ttrain's rmse: 0.712806\tvalid's rmse: 0.697543\nEarly stopping, best iteration is:\n[15]\ttrain's rmse: 0.731037\tvalid's rmse: 0.694924\nFold 1 validation R2 score: 0.004591030018426645\nFold 2: Training LightGBM\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 19465\n[LightGBM] [Info] Number of data points in the train set: 894432, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (68.24 MB) transferred to GPU in 0.054944 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.002577\nTraining until validation scores don't improve for 100 rounds\n[50]\ttrain's rmse: 0.707196\tvalid's rmse: 0.764873\n[100]\ttrain's rmse: 0.698099\tvalid's rmse: 0.766945\nEarly stopping, best iteration is:\n[22]\ttrain's rmse: 0.713943\tvalid's rmse: 0.764279\nFold 2 validation R2 score: 0.0035395242891257395\nFold 3: Training LightGBM\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 19465\n[LightGBM] [Info] Number of data points in the train set: 892496, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (68.09 MB) transferred to GPU in 0.055824 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.003925\nTraining until validation scores don't improve for 100 rounds\n[50]\ttrain's rmse: 0.733411\tvalid's rmse: 0.663143\n[100]\ttrain's rmse: 0.72508\tvalid's rmse: 0.663784\nEarly stopping, best iteration is:\n[24]\ttrain's rmse: 0.738825\tvalid's rmse: 0.66286\nFold 3 validation R2 score: 0.004100784051716322\nFold 4: Training LightGBM\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 19465\n[LightGBM] [Info] Number of data points in the train set: 893464, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (68.17 MB) transferred to GPU in 0.057593 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.002435\nTraining until validation scores don't improve for 100 rounds\n[50]\ttrain's rmse: 0.713412\tvalid's rmse: 0.739966\n[100]\ttrain's rmse: 0.704337\tvalid's rmse: 0.740076\n[150]\ttrain's rmse: 0.696326\tvalid's rmse: 0.741288\nEarly stopping, best iteration is:\n[78]\ttrain's rmse: 0.708273\tvalid's rmse: 0.739733\nFold 4 validation R2 score: 0.0038104559393831172\nSaved all models to stacking_models.pkl\nCross-validation R2 scores: [0.004633637729981821, 0.004591030018426645, 0.0035395242891257395, 0.004100784051716322, 0.0038104559393831172]\nMean R2 score: 0.004135086405726729, Std: 0.00042841424504274684\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Second Version: Model Loading","metadata":{}},{"cell_type":"code","source":"# 이거는 catboost 아닐때\n\n# # Load the model from the saved file\n\n# model_path = '/kaggle/input/jsmodel-chan2'\n# model_name = 'lgb_model'\n# models = []\n# models.append(joblib.load(f'{model_path}/{model_name}.pkl'))\n\n# print(f\"Loaded model from the saved file.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 이거는 catboost 일때\n\nmodel_path = '/kaggle/working/models2'\nmodel_name = 'stacking_models'\n\nstacking_models = joblib.load(f'{model_path}/{model_name}.pkl') #dict\n\nlgb_models = stacking_models['lgb_models'] #lst\ncatboost_models = stacking_models['catboost_models'] #lst\n\nprint(f\"Loaded model from the saved file.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:26:29.478682Z","iopub.execute_input":"2024-12-29T11:26:29.479603Z","iopub.status.idle":"2024-12-29T11:26:30.465027Z","shell.execute_reply.started":"2024-12-29T11:26:29.479545Z","shell.execute_reply":"2024-12-29T11:26:30.464067Z"}},"outputs":[{"name":"stdout","text":"Loaded model from the saved file.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"lags_ : pl.DataFrame | None = None\n\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    global lags_\n    if lags is not None:\n        lags_ = lags\n\n    predictions = test.select(\n        'row_id',\n        pl.lit(0.0).alias('responder_6')\n    )\n\n    # feat = test[FEAT_COLS].to_pandas()\n    # feat = test[FEAT_COLS + ['weight']].to_pandas()\n    \n    # model = models[0]\n    # pred = model.predict(feat)\n    \n    # pred = [model.predict(feat) for model in models]\n    # pred = np.mean(pred, axis=0)\n    \n    feat_cat = test[FEAT_COLS + ['symbol_id']].to_pandas()\n    feat_cat = feat_cat.fillna('NaN').astype(str)\n    pred_cat= [model.predict(feat_cat) for model in catboost_models]\n    pred_cat = np.mean(pred_cat, axis=0)\n\n    # feat_lgb: LightGBM 모델의 feature\n    # `pred_cat`을 `test`의 feature로 추가\n    feat_lgb = test[FEAT_COLS].to_pandas()\n    feat_lgb['pred_cat'] = pred_cat  # pred_cat을 새로운 column으로 추가\n    \n    pred_lgb = [model.predict(feat_lgb) for model in lgb_models]\n    pred_lgb = np.mean(pred_lgb, axis=0)\n\n    # 최종 예측값 계산\n    pred = pred_cat * 0.5 + pred_lgb * 0.5\n    \n    predictions = predictions.with_columns(pl.Series('responder_6', pred.ravel()))\n    print(predictions)\n    \n    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n    \n    assert list(predictions.columns) == ['row_id', 'responder_6']\n    assert len(predictions) == len(test)\n    \n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:48:49.264274Z","iopub.execute_input":"2024-12-29T11:48:49.264958Z","iopub.status.idle":"2024-12-29T11:48:49.272834Z","shell.execute_reply.started":"2024-12-29T11:48:49.264923Z","shell.execute_reply":"2024-12-29T11:48:49.271791Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T11:48:49.630470Z","iopub.execute_input":"2024-12-29T11:48:49.631172Z","iopub.status.idle":"2024-12-29T11:48:49.716532Z","shell.execute_reply.started":"2024-12-29T11:48:49.631138Z","shell.execute_reply":"2024-12-29T11:48:49.714217Z"}},"outputs":[{"name":"stdout","text":"shape: (39, 2)\n┌────────┬─────────────┐\n│ row_id ┆ responder_6 │\n│ ---    ┆ ---         │\n│ i64    ┆ f64         │\n╞════════╪═════════════╡\n│ 0      ┆ 0.192078    │\n│ 1      ┆ 0.191906    │\n│ 2      ┆ 0.194858    │\n│ 3      ┆ 0.195911    │\n│ 4      ┆ 0.194446    │\n│ …      ┆ …           │\n│ 34     ┆ 0.195014    │\n│ 35     ┆ 0.193742    │\n│ 36     ┆ 0.202089    │\n│ 37     ┆ 0.194819    │\n│ 38     ┆ 0.195359    │\n└────────┴─────────────┘\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}