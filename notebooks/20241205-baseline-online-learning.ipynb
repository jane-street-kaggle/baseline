{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle api reference\n",
    "https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from typing import TYPE_CHECKING, Optional, List, Dict, Any, Callable, Union, Tuple\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gc\n",
    "import optuna\n",
    "import dill\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandas 2.2.3, polars 1.6.0, numpy 2.0.2, lightgbm 4.5.0, xgboost 2.1.1, optuna 4.0.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f\"pandas {pd.__version__}, polars {pl.__version__}, numpy {np.__version__}, lightgbm {lgb.__version__}, xgboost {xgb.__version__}, optuna {optuna.__version__}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "if not IS_KAGGLE: \n",
    "    os.chdir('./kaggle')\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "BASE_PATH = '/kaggle/input/jane-street-real-time-market-data-forecasting' if IS_KAGGLE else './data'\n",
    "MODEL_PATH = '/kaggle/working' if IS_KAGGLE else './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, './data', './models')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IS_KAGGLE, BASE_PATH, MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TYPE_CHECKING:\n",
    "    from __main__ import Config, ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_metric(y_true, y_pred, weights=None):\n",
    "    \"\"\"Calculate weighted R2 score\"\"\"\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # If weights is None, use uniform weights\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(y_true)\n",
    "    else:\n",
    "        weights = weights.ravel()\n",
    "\n",
    "    numerator = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "    denominator = np.sum(weights * (y_true ** 2))\n",
    "    r2_score = 1 - (numerator / denominator)\n",
    "    return 'r2', r2_score, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def versioned_function(version: str, description: str = \"\"):\n",
    "    \"\"\"Function versioning decorator\"\"\"\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return func(*args, **kwargs)\n",
    "        \n",
    "        wrapper.version = version\n",
    "        wrapper.description = description\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass NeuralNetworkModel(BaseModel):\\n    def _register_custom_metrics(self):\\n        # PyTorch/TensorFlow에서는 metrics를 모델 컴파일 시 등록\\n        self.metrics = list(self.config.custom_metrics.values())\\n    \\n    def fit(self, train_data: Tuple[np.ndarray, np.ndarray], \\n           val_data: Optional[Tuple[np.ndarray, np.ndarray]] = None):\\n        train_X, train_y = train_data\\n        \\n        # PyTorch example\\n        self.model = torch.nn.Sequential(\\n            torch.nn.Linear(train_X.shape[1], 64),\\n            torch.nn.ReLU(),\\n            torch.nn.Linear(64, 1)\\n        )\\n        \\n        optimizer = torch.optim.Adam(self.model.parameters(), \\n                                   lr=self.config.params.get(\\'learning_rate\\', 0.001))\\n        \\n        # Training loop with custom metrics\\n        for epoch in range(self.config.params.get(\\'epochs\\', 10)):\\n            self.model.train()\\n            # ... training implementation ...\\n            \\n            if val_data is not None:\\n                self.model.eval()\\n                # ... validation implementation ...\\n    \\n    def predict(self, X: np.ndarray) -> np.ndarray:\\n        self.model.eval()\\n        with torch.no_grad():\\n            X_tensor = torch.FloatTensor(X)\\n            return self.model(X_tensor).numpy()\\n\\nclass EnsembleModel(BaseModel):\\n    def __init__(self, config: \"ModelConfig\", models: List[BaseModel]):\\n        super().__init__(config)\\n        self.models = models\\n    \\n    def _register_custom_metrics(self):\\n        # 각 모델의 custom metrics는 이미 등록되어 있음\\n        pass\\n    \\n    def fit(self, train_data: Tuple[np.ndarray, np.ndarray], \\n           val_data: Optional[Tuple[np.ndarray, np.ndarray]] = None):\\n        for model in self.models:\\n            model.fit(train_data, val_data)\\n    \\n    def predict(self, X: np.ndarray) -> np.ndarray:\\n        predictions = [model.predict(X) for model in self.models]\\n        # Weighted average if weights are specified in config\\n        weights = self.config.params.get(\\'weights\\', None)\\n        if weights is not None:\\n            return np.average(predictions, axis=0, weights=weights)\\n        return np.mean(predictions, axis=0)\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BaseModel:\n",
    "    \"\"\"Base model class for easy extension\"\"\"\n",
    "    def __init__(self, config: \"ModelConfig\"):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self._register_custom_metrics()\n",
    "    \n",
    "    def _register_custom_metrics(self):\n",
    "        \"\"\"Register custom metrics if needed\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, train_data: Tuple[np.ndarray, np.ndarray], \n",
    "           val_data: Optional[Tuple[np.ndarray, np.ndarray]] = None):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        with open(path, 'wb') as f:\n",
    "            dill.dump(self.model, f)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.model = dill.load(f)\n",
    "\n",
    "class LightGBMModel(BaseModel):\n",
    "    def _register_custom_metrics(self):\n",
    "        \"\"\"Register custom metrics for LightGBM\"\"\"\n",
    "        # Instead of registering metrics directly, we'll add them to params\n",
    "        if self.config.custom_metrics:\n",
    "            self.config.params['metric'] = list(self.config.custom_metrics.keys())\n",
    "    \n",
    "    def fit(self, train_data: Tuple[np.ndarray, np.ndarray, np.ndarray], \n",
    "           val_data: Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]] = None):\n",
    "        train_X, train_y, train_w = train_data\n",
    "        print(f\"\\nMemory usage before training:\")\n",
    "        print(f\"train_X: {train_X.nbytes / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        print(f\"train_y: {train_y.nbytes / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        if train_w is not None:\n",
    "            print(f\"train_w: {train_w.nbytes / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        \n",
    "        train_set = lgb.Dataset(train_X, train_y, weight=train_w, free_raw_data=False, params={'feature_pre_filter': False})\n",
    "        \n",
    "        del train_X, train_y, train_w, train_data\n",
    "        gc.collect()\n",
    "        \n",
    "        val_set = None\n",
    "        if val_data is not None:\n",
    "            val_X, val_y, val_w = val_data\n",
    "            print(f\"\\nValidation data memory usage:\")\n",
    "            print(f\"val_X: {val_X.nbytes / 1024 / 1024 / 1024:.2f} GB\")\n",
    "            print(f\"val_y: {val_y.nbytes / 1024 / 1024 / 1024:.2f} GB\")\n",
    "            if val_w is not None:\n",
    "                print(f\"val_w: {val_w.nbytes / 1024 / 1024 / 1024:.2f} GB\")\n",
    "            val_set = lgb.Dataset(val_X, val_y, weight=val_w, free_raw_data=False, params={'feature_pre_filter': False})\n",
    "            \n",
    "            del val_X, val_y, val_w, val_data\n",
    "            gc.collect()\n",
    "        \n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "        \n",
    "        self.model = lgb.train(\n",
    "            self.config.params,\n",
    "            train_set,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[val_set] if val_set else None,\n",
    "            valid_names=['valid'] if val_set else None,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class XGBoostModel(BaseModel):\n",
    "    def _register_custom_metrics(self):\n",
    "        \"\"\"Register custom metrics for XGBoost\"\"\"\n",
    "        if self.config.custom_metrics:\n",
    "            self.config.params['custom_metric'] = list(self.config.custom_metrics.values())\n",
    "    \n",
    "    def fit(self, train_data: Tuple[np.ndarray, np.ndarray, np.ndarray], \n",
    "           val_data: Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]] = None):\n",
    "        \"\"\"Train XGBoost model\"\"\"\n",
    "        train_X, train_y, train_w = train_data\n",
    "        train_set = xgb.DMatrix(train_X, train_y, weight=train_w)\n",
    "        \n",
    "        del train_X, train_y, train_w\n",
    "        gc.collect()\n",
    "        \n",
    "        watchlist = [(train_set, 'train')]\n",
    "        if val_data is not None:\n",
    "            val_X, val_y, val_w = val_data\n",
    "            val_set = xgb.DMatrix(val_X, val_y, weight=val_w)\n",
    "            watchlist.append((val_set, 'valid'))\n",
    "            \n",
    "            del val_X, val_y, val_w\n",
    "            gc.collect()\n",
    "        \n",
    "        self.model = xgb.train(\n",
    "            self.config.params,\n",
    "            train_set,\n",
    "            num_boost_round=1000,\n",
    "            evals=watchlist,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        del train_set\n",
    "        if val_data is not None:\n",
    "            del val_set\n",
    "        gc.collect()\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.model.predict(xgb.DMatrix(X))\n",
    "\n",
    "# Example model implementations\n",
    "\"\"\"\n",
    "class EnsembleModel(BaseModel):\n",
    "    def __init__(self, config: \"ModelConfig\", models: List[BaseModel]):\n",
    "        super().__init__(config)\n",
    "        self.models = models\n",
    "    \n",
    "    def _register_custom_metrics(self):\n",
    "        # 각 모델의 custom metrics는 이미 등록되어 있음\n",
    "        pass\n",
    "    \n",
    "    def fit(self, train_data: Tuple[np.ndarray, np.ndarray], \n",
    "           val_data: Optional[Tuple[np.ndarray, np.ndarray]] = None):\n",
    "        for model in self.models:\n",
    "            model.fit(train_data, val_data)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        predictions = [model.predict(X) for model in self.models]\n",
    "        # Weighted average if weights are specified in config\n",
    "        weights = self.config.params.get('weights', None)\n",
    "        if weights is not None:\n",
    "            return np.average(predictions, axis=0, weights=weights)\n",
    "        return np.mean(predictions, axis=0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    \"\"\"Basic MLP architecture\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "class NeuralNetworkModel(BaseModel):\n",
    "    def _register_custom_metrics(self):\n",
    "        \"\"\"Register custom metrics for Neural Network\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, train_data: Tuple[np.ndarray, np.ndarray, np.ndarray], \n",
    "           val_data: Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]] = None):\n",
    "        \"\"\"Train Neural Network model\"\"\"\n",
    "        # Clear GPU cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        train_X, train_y, train_w = train_data\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        train_X = torch.FloatTensor(train_X)\n",
    "        train_y = torch.FloatTensor(train_y.reshape(-1))\n",
    "        train_w = torch.FloatTensor(train_w.reshape(-1)) if train_w is not None else None\n",
    "        \n",
    "        # Create model\n",
    "        if self.model is None:\n",
    "            self.model = MLPModel(\n",
    "                input_dim=train_X.shape[1],\n",
    "                hidden_dims=self.config.params.get('hidden_dims', [256, 128, 64]),\n",
    "                dropout=self.config.params.get('dropout', 0.1)\n",
    "            )\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"\\nUsing device: {device}\")\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Create data loaders with pin_memory\n",
    "        train_dataset = TensorDataset(train_X, train_y) if train_w is None else TensorDataset(train_X, train_y, train_w)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config.params.get('batch_size', 256),\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=24\n",
    "        )\n",
    "        print(f\"Number of training batches: {len(train_loader)}\")\n",
    "        \n",
    "        val_loader = None\n",
    "        if val_data is not None:\n",
    "            val_X, val_y, val_w = val_data\n",
    "            val_X = torch.FloatTensor(val_X)\n",
    "            val_y = torch.FloatTensor(val_y.reshape(-1))\n",
    "            val_w = torch.FloatTensor(val_w.reshape(-1)) if val_w is not None else None\n",
    "            \n",
    "            val_dataset = TensorDataset(val_X, val_y) if val_w is None else TensorDataset(val_X, val_y, val_w)\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=self.config.params.get('batch_size', 256),\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "                num_workers=24\n",
    "            )\n",
    "            print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "        \n",
    "        # Setup training\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.params.get('learning_rate', 1e-3),\n",
    "            weight_decay=self.config.params.get('weight_decay', 1e-5)\n",
    "        )\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = self.config.params.get('patience', 10)\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        epochs = self.config.params.get('epochs', 100)\n",
    "        print(\"\\nStarting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if len(batch) == 3:  # with weights\n",
    "                    x_batch, y_batch, w_batch = [b.to(device, non_blocking=True) for b in batch]\n",
    "                    y_pred = self.model(x_batch)\n",
    "                    loss = (criterion(y_pred, y_batch) * w_batch).mean()\n",
    "                else:  # without weights\n",
    "                    x_batch, y_batch = [b.to(device, non_blocking=True) for b in batch]\n",
    "                    y_pred = self.model(x_batch)\n",
    "                    loss = criterion(y_pred, y_batch).mean()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            train_loss /= batch_count\n",
    "            \n",
    "            # Validation\n",
    "            if val_loader is not None:\n",
    "                self.model.eval()\n",
    "                val_loss = 0\n",
    "                val_batch_count = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        if len(batch) == 3:  # with weights\n",
    "                            x_batch, y_batch, w_batch = [b.to(device, non_blocking=True) for b in batch]\n",
    "                            y_pred = self.model(x_batch)\n",
    "                            loss = (criterion(y_pred, y_batch) * w_batch).mean()\n",
    "                        else:  # without weights\n",
    "                            x_batch, y_batch = [b.to(device, non_blocking=True) for b in batch]\n",
    "                            y_pred = self.model(x_batch)\n",
    "                            loss = criterion(y_pred, y_batch).mean()\n",
    "                        val_loss += loss.item()\n",
    "                        val_batch_count += 1\n",
    "                \n",
    "                val_loss /= val_batch_count\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                # Print progress every epoch\n",
    "                print(f'Epoch {epoch+1}/{epochs}: train_loss = {train_loss:.4f}, val_loss = {val_loss:.4f}, best_val_loss = {best_val_loss:.4f}')\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f'\\nEarly stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}/{epochs}: train_loss = {train_loss:.4f}')\n",
    "            \n",
    "            # Periodic cache clearing\n",
    "            if epoch % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"\\nTraining completed!\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del train_X, train_y, train_w, train_loader\n",
    "        if val_data is not None:\n",
    "            del val_X, val_y, val_w, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions in batches with memory optimization\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        batch_size = 128\n",
    "        device = next(self.model.parameters()).device\n",
    "    \n",
    "        # Debug info\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(\"X type:\", type(X))\n",
    "    \n",
    "        # Ensure X is a pure numpy array\n",
    "        if hasattr(X, 'to_numpy'):\n",
    "            X = X.to_numpy()\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "    \n",
    "        print(\"After conversion - X shape:\", X.shape)\n",
    "        print(\"After conversion - X type:\", type(X))\n",
    "        print(\"After conversion - X dtype:\", X.dtype)\n",
    "    \n",
    "        # Clear GPU cache before prediction\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        try:\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch = torch.FloatTensor(X[i:i+batch_size]).to(device)\n",
    "                with torch.no_grad():\n",
    "                    batch_pred = self.model(batch).cpu().numpy()\n",
    "                predictions.append(batch_pred)\n",
    "    \n",
    "                del batch\n",
    "                if i % (batch_size * 10) == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "            final_predictions = np.concatenate(predictions)\n",
    "    \n",
    "            del predictions\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            return final_predictions\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(\"GPU memory error occurred. Attempting with smaller batch size...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return self.predict_with_batch_size(X, batch_size // 2)\n",
    "    \n",
    "    def predict_with_batch_size(self, X: np.ndarray, batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Helper method for recursive batch size reduction\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        device = next(self.model.parameters()).device\n",
    "        \n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = torch.FloatTensor(X[i:i+batch_size]).to(device)\n",
    "            with torch.no_grad():\n",
    "                batch_pred = self.model(batch).cpu().numpy()\n",
    "            predictions.append(batch_pred)\n",
    "            del batch\n",
    "            \n",
    "        return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, config: \"Config\"):\n",
    "        self.config = config\n",
    "        self.date_ranges = None\n",
    "        self.test_data = None\n",
    "        self.features = None\n",
    "        self.preprocessor = None\n",
    "        self.feature_generator = None\n",
    "    \n",
    "    def load_data(self) -> Tuple[Dict[int, Tuple[int, int]], Optional[pl.DataFrame]]:\n",
    "        \"\"\"Load test data and get date ranges for train partitions\"\"\"\n",
    "        try:\n",
    "            # Load test data\n",
    "            self.test_data = pl.read_parquet(f\"{BASE_PATH}/test.parquet\")\n",
    "            \n",
    "            # Get date ranges for each partition lazily\n",
    "            self.date_ranges = {}\n",
    "            partition_range = self.config.partition_range or range(10)  # Default to all partitions\n",
    "            \n",
    "            for i in partition_range:\n",
    "                date_range_df = pl.scan_parquet(\n",
    "                    f\"{BASE_PATH}/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "                ).select('date_id')\n",
    "                \n",
    "                # 수정된 부분: min과 max에 각각 다른 alias 지정\n",
    "                date_stats = date_range_df.select([\n",
    "                    pl.col('date_id').min().alias('min_date'),\n",
    "                    pl.col('date_id').max().alias('max_date')\n",
    "                ]).collect()\n",
    "                \n",
    "                self.date_ranges[i] = (\n",
    "                    date_stats[0, 'min_date'],  # alias로 접근\n",
    "                    date_stats[0, 'max_date']   # alias로 접근\n",
    "                )\n",
    "            \n",
    "            print(\"\\nTrain Data Date Ranges per Partition:\")\n",
    "            for partition, (min_date, max_date) in self.date_ranges.items():\n",
    "                print(f\"Partition {partition}: date_id {min_date} to {max_date}\")\n",
    "            \n",
    "            return self.date_ranges, self.test_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def _load_partition_data_by_datarange(self, date_range: Optional[Tuple[int, int]] = None) -> pl.DataFrame:\n",
    "        \"\"\"Load and return train data for specified date range\"\"\"\n",
    "        try:\n",
    "            relevant_partitions = self._get_relevant_partitions(date_range)\n",
    "            print(f\"\\nLoading train data from partitions: {relevant_partitions}\")\n",
    "            \n",
    "            # Load relevant partitions\n",
    "            train_parts = []\n",
    "            for i in relevant_partitions:\n",
    "                part_df = pl.scan_parquet(  # read_parquet 대신 scan_parquet 사용\n",
    "                    f\"{BASE_PATH}/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "                )\n",
    "                \n",
    "                # Filter by date range if specified\n",
    "                start_date, end_date = date_range\n",
    "                part_df = part_df.filter(\n",
    "                    (pl.col('date_id') >= start_date) & \n",
    "                    (pl.col('date_id') <= end_date)\n",
    "                )\n",
    "                train_parts.append(part_df)\n",
    " \n",
    "            # Concatenate parts\n",
    "            train_data = pl.concat(train_parts, how='vertical').collect()\n",
    " \n",
    "            # Clear memory\n",
    "            del train_parts\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"Loaded train data shape: {train_data.shape}\")\n",
    "            print(f\"Date range in loaded data: {train_data['date_id'].min()} to {train_data['date_id'].max()}\")\n",
    "            \n",
    "            return train_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading train data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _process_and_generate_features(self, raw_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Apply preprocessing and generate features from raw DataFrame\"\"\"\n",
    "        try:\n",
    "            # Preprocessing\n",
    "            if self.preprocessor:\n",
    "                processed_df = self.preprocessor(raw_df)\n",
    "                del raw_df\n",
    "                gc.collect()\n",
    "            else:\n",
    "                processed_df = raw_df\n",
    "\n",
    "            # Feature generation\n",
    "            if self.feature_generator:\n",
    "                featured_df = self.feature_generator(processed_df)\n",
    "                del processed_df\n",
    "                gc.collect()\n",
    "                self.features = [col for col in featured_df.columns\n",
    "                                if col.startswith('feature_')]\n",
    "                return featured_df\n",
    "            \n",
    "            return processed_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data processing and feature generation: {e}\")\n",
    "            return raw_df\n",
    "    \n",
    "    def prepare_data(self, date_range: Optional[Tuple[int, int]] = None, \n",
    "                    preprocessor: Optional[Callable] = None,\n",
    "                    feature_generator: Optional[Callable] = None) -> pl.DataFrame:\n",
    "        \"\"\"Prepare data with custom preprocessing and feature generation\"\"\"\n",
    "        try:\n",
    "            self.preprocessor = preprocessor\n",
    "            self.feature_generator = feature_generator\n",
    "            \n",
    "            # Load data\n",
    "            raw_df = self._load_partition_data_by_datarange(date_range)\n",
    "            if raw_df is None:\n",
    "                raise ValueError(\"Failed to load partition data\")\n",
    "\n",
    "            # Process and generate features\n",
    "            return self._process_and_generate_features(raw_df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prepare_data: {e}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            gc.collect()\n",
    "    \n",
    "    def _get_relevant_partitions(self, date_range: Tuple[int, int]) -> List[int]:\n",
    "        \"\"\"Get relevant partitions for a given date range\"\"\"\n",
    "        start_date, end_date = date_range\n",
    "        relevant_partitions = []\n",
    "        for partition, (min_date, max_date) in self.date_ranges.items():\n",
    "            if (start_date <= max_date) and (end_date >= min_date):\n",
    "                relevant_partitions.append(partition)\n",
    "        return relevant_partitions\n",
    "\n",
    "    def generate_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Generate features for a specific DataFrame\"\"\"\n",
    "        if self.feature_generator:\n",
    "            df = self.feature_generator(df)\n",
    "            self.features = [col for col in df.columns\n",
    "                            if col.startswith('feature_')]\n",
    "            return df\n",
    "        return df\n",
    "    \n",
    "    def get_feature_data(self, df: pl.DataFrame) -> Tuple[np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        \"\"\"Extract features, target, and weights\"\"\"\n",
    "        missing_features = [f for f in self.features if f not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features in input data: {missing_features}\")\n",
    "\n",
    "        X = df.select(self.features).to_numpy()\n",
    "        y = df.select('responder_6').to_numpy() if 'responder_6' in df.columns else None\n",
    "        w = df.select('weight').to_numpy() if 'weight' in df.columns else None\n",
    "\n",
    "        return X, y, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SplitStrategy(ABC):\n",
    "    \"\"\"Base class for split strategies\"\"\"\n",
    "    def __init__(self, test_ratio: float = 0.2):\n",
    "        self.test_ratio = test_ratio\n",
    "    \n",
    "    def get_holdout_test(self, partition_date_ranges: Dict[int, Tuple[int, int]]) -> Tuple[int, int]:\n",
    "        \"\"\"Calculate holdout test date range based on partition date ranges\n",
    "        Args:\n",
    "            partition_date_ranges: Dict[int, Tuple[int, int]]: {partition_id: (min_date, max_date)}\n",
    "        Returns:\n",
    "            Tuple[Tuple[int, int], Tuple[int, int]]: ((train_start_date, train_end_date), (test_start_date, test_end_date))\n",
    "        \"\"\"\n",
    "        # Get all unique dates from partition ranges\n",
    "        all_dates = set()\n",
    "        for _, (min_date, max_date) in partition_date_ranges.items():\n",
    "            all_dates.update(range(min_date, max_date + 1))\n",
    "        \n",
    "        unique_dates = sorted(list(all_dates))\n",
    "        split_idx = int(len(unique_dates) * (1 - self.test_ratio))\n",
    "        split_date = unique_dates[split_idx]\n",
    "        \n",
    "        print(\"\\nHoldout Test Split Info:\")\n",
    "        print(f\"Total unique dates: {len(unique_dates)}\")\n",
    "        print(f\"Train dates range: {unique_dates[0]} - {split_date}\")\n",
    "        print(f\"Test dates range: {unique_dates[split_idx + 1]} - {unique_dates[-1]}\")\n",
    "        \n",
    "        return (unique_dates[0], split_date), (unique_dates[split_idx + 1], unique_dates[-1])\n",
    "    \n",
    "    def visualize_splits(self, date_range: Tuple[int, int], figsize: Tuple[int, int] = (15, 8)):\n",
    "        \"\"\"Visualize the splits\n",
    "        Args:\n",
    "            date_range: Tuple[int, int]: (start_date, end_date)\n",
    "            figsize: Tuple[int, int]: Figure size for matplotlib\n",
    "        \"\"\"\n",
    "        splits = self.split(date_range)\n",
    "        n_splits = len(splits)\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot splits\n",
    "        for idx, ((train_start, train_end), (val_start, val_end)) in enumerate(splits):\n",
    "            # Plot training period\n",
    "            plt.barh(y=idx, \n",
    "                    width=train_end - train_start, \n",
    "                    left=train_start, \n",
    "                    height=0.3, \n",
    "                    color='royalblue', \n",
    "                    alpha=0.6,\n",
    "                    label='Train' if idx == 0 else \"\")\n",
    "            \n",
    "            # Plot validation period\n",
    "            plt.barh(y=idx, \n",
    "                    width=val_end - val_start, \n",
    "                    left=val_start, \n",
    "                    height=0.3, \n",
    "                    color='coral', \n",
    "                    alpha=0.6,\n",
    "                    label='Validation' if idx == 0 else \"\")\n",
    "            \n",
    "            # If gap exists (for PurgedGroupTimeSeriesSplit)\n",
    "            if hasattr(self, 'group_gap') and self.group_gap > 0:\n",
    "                plt.barh(y=idx, \n",
    "                        width=val_start - train_end, \n",
    "                        left=train_end, \n",
    "                        height=0.3, \n",
    "                        color='lightgray', \n",
    "                        alpha=0.3,\n",
    "                        label='Gap' if idx == 0 else \"\")\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.yticks(range(n_splits), [f'Split {i}' for i in range(n_splits)])\n",
    "        plt.xlabel('Date ID')\n",
    "        plt.ylabel('CV Iteration')\n",
    "        plt.title(f'{self.__class__.__name__} - {n_splits} Splits')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add date range info\n",
    "        plt.text(0.98, -0.15, \n",
    "                f'Date Range: {date_range[0]} to {date_range[1]}',\n",
    "                horizontalalignment='right',\n",
    "                transform=plt.gca().transAxes,\n",
    "                fontsize=10,\n",
    "                alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def split(self, data: pl.DataFrame) -> List[Tuple[pl.DataFrame, pl.DataFrame]]:\n",
    "        \"\"\"Split remaining data into train/val sets for cross validation\n",
    "        Returns:\n",
    "            List of (train, val) DataFrame tuples\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class TimeBasedSplit(SplitStrategy):\n",
    "    def __init__(self, train_ratio: float = 0.75, test_ratio: float = 0.2):\n",
    "        super().__init__(test_ratio)\n",
    "        self.train_ratio = train_ratio\n",
    "    \n",
    "    def split(self, date_range: Tuple[int, int]) -> List[Tuple[Tuple[int, int], Tuple[int, int]]]:\n",
    "        \"\"\"Single split based on date range\n",
    "        Args:\n",
    "            date_range: Tuple[int, int]: (start_date, end_date)\n",
    "        Returns:\n",
    "            List[Tuple[Tuple[int, int], Tuple[int, int]]]: [((train_start_date, train_end_date), (val_start_date, val_end_date))]\n",
    "        \"\"\"\n",
    "        start_date, end_date = date_range\n",
    "        total_days = end_date - start_date + 1\n",
    "        split_idx = start_date + int(total_days * self.train_ratio) - 1\n",
    "        \n",
    "        print(\"\\nTime Based Split Info:\")\n",
    "        print(f\"Train dates range: {start_date} - {split_idx}\")\n",
    "        print(f\"Val dates range: {split_idx + 1} - {end_date}\")\n",
    "        \n",
    "        return [((start_date, split_idx), (split_idx + 1, end_date))]\n",
    "\n",
    "class TimeSeriesKFold(SplitStrategy):\n",
    "    def __init__(self, n_splits: int = 5, test_ratio: float = 0.2):\n",
    "        super().__init__(test_ratio)\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def split(self, date_range: Tuple[int, int]) -> List[Tuple[Tuple[int, int], Tuple[int, int]]]:\n",
    "        \"\"\"Multiple splits based on date range\n",
    "        Args:\n",
    "            date_range: Tuple[int, int]: (start_date, end_date)\n",
    "        Returns:\n",
    "            List[Tuple[Tuple[int, int], Tuple[int, int]]]: List of ((train_start_date, train_end_date), (val_start_date, val_end_date))\n",
    "        \"\"\"\n",
    "        start_date, end_date = date_range\n",
    "        total_days = end_date - start_date + 1\n",
    "        splits = []\n",
    "        \n",
    "        # Calculate initial training size and increment\n",
    "        initial_train_days = total_days // (self.n_splits + 1)\n",
    "        remaining_days = total_days - initial_train_days\n",
    "        val_size = remaining_days // self.n_splits\n",
    "        \n",
    "        print(f\"\\nTime Series {self.n_splits}-Fold Split Info:\")\n",
    "        print(f\"Total days: {total_days}\")\n",
    "        print(f\"Initial train size: {initial_train_days} days\")\n",
    "        print(f\"Validation size: ~{val_size} days per fold\")\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            train_end = start_date + initial_train_days + (i * val_size) - 1\n",
    "            val_end = train_end + val_size\n",
    "            if i == self.n_splits - 1:  # Last fold uses all remaining dates\n",
    "                val_end = end_date\n",
    "            \n",
    "            print(f\"\\nFold {i+1}:\")\n",
    "            print(f\"Train dates range: {start_date} - {train_end}\")\n",
    "            print(f\"Val dates range: {train_end + 1} - {val_end}\")\n",
    "            \n",
    "            splits.append(((start_date, train_end), (train_end + 1, val_end)))\n",
    "        \n",
    "        return splits\n",
    "\n",
    "class PurgedGroupTimeSeriesSplit(SplitStrategy):\n",
    "    def __init__(self, n_splits: int = 5, \n",
    "                 max_train_group_size: int = np.inf,\n",
    "                 max_test_group_size: int = np.inf,\n",
    "                 group_gap: int = None,\n",
    "                 test_ratio: float = 0.2):\n",
    "        super().__init__(test_ratio)\n",
    "        self.n_splits = n_splits\n",
    "        self.max_train_group_size = max_train_group_size \n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.group_gap = group_gap if group_gap is not None else 0\n",
    "\n",
    "    def split(self, date_range: Tuple[int, int]) -> List[Tuple[Tuple[int, int], Tuple[int, int]]]:\n",
    "        \"\"\"Multiple splits based on date range with gap between train and validation\n",
    "        Args:\n",
    "            date_range: Tuple[int, int]: (start_date, end_date)\n",
    "        Returns:\n",
    "            List[Tuple[Tuple[int, int], Tuple[int, int]]]: List of ((train_start_date, train_end_date), (val_start_date, val_end_date))\n",
    "        \"\"\"\n",
    "        start_date, end_date = date_range\n",
    "        total_days = end_date - start_date + 1\n",
    "        splits = []\n",
    "        \n",
    "        # Calculate number of groups and group size\n",
    "        n_groups = total_days\n",
    "        n_folds = self.n_splits + 1\n",
    "        \n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                f\"Cannot have number of folds={n_folds} greater than\"\n",
    "                f\" the number of groups={n_groups}\")\n",
    "        \n",
    "        # Calculate test group size\n",
    "        group_test_size = min(n_groups // n_folds, self.max_test_group_size)\n",
    "        \n",
    "        # Calculate test start positions\n",
    "        group_test_starts = range(n_groups - self.n_splits * group_test_size,\n",
    "                                n_groups, group_test_size)\n",
    "        \n",
    "        print(f\"\\nPurged Group Time Series {self.n_splits}-Fold Split Info:\")\n",
    "        print(f\"Total days: {total_days}\")\n",
    "        print(f\"Group test size: {group_test_size}\")\n",
    "        print(f\"Group gap: {self.group_gap}\")\n",
    "        \n",
    "        for group_test_start in group_test_starts:\n",
    "            # Calculate train period\n",
    "            group_st = max(0, group_test_start - self.group_gap - self.max_train_group_size)\n",
    "            train_start = start_date + group_st\n",
    "            train_end = start_date + group_test_start - self.group_gap\n",
    "            \n",
    "            # Calculate test period\n",
    "            test_start = start_date + group_test_start + self.group_gap\n",
    "            test_end = min(start_date + group_test_start + group_test_size, end_date)\n",
    "            \n",
    "            print(f\"\\nFold:\")\n",
    "            print(f\"Train dates range: {train_start} - {train_end}\")\n",
    "            print(f\"Gap dates range: {train_end + 1} - {test_start - 1}\")\n",
    "            print(f\"Test dates range: {test_start} - {test_end}\")\n",
    "            \n",
    "            splits.append(((train_start, train_end), (test_start, test_end)))\n",
    "        \n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Handler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationHandler:\n",
    "    def __init__(self, config: \"Config\", model_class: type):\n",
    "        self.config = config\n",
    "        self.model_class = model_class\n",
    "    \n",
    "    def get_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:\n",
    "        \"\"\"Define search space for each model type\"\"\"\n",
    "        if self.config.model.name == 'lightgbm':\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 16, 96),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "                'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "            }\n",
    "        \"\"\"\n",
    "        elif self.config.model.name == 'xgboost':\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "                'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "            }\n",
    "        \"\"\"\n",
    "        return {}\n",
    "    \n",
    "    def objective(self, trial: optuna.Trial, train_data: Tuple[np.ndarray, np.ndarray], \n",
    "                 val_data: Tuple[np.ndarray, np.ndarray]) -> float:\n",
    "        \"\"\"Optimization objective\"\"\"\n",
    "        params = self.get_search_space(trial)\n",
    "        self.config.model.params.update(params)\n",
    "        \n",
    "        model = self.model_class(self.config.model)\n",
    "        model.fit(train_data, val_data)\n",
    "        \n",
    "        val_X, val_y = val_data\n",
    "        predictions = model.predict(val_X)\n",
    "        \n",
    "        return np.mean((predictions - val_y) ** 2) ** 0.5\n",
    "    \n",
    "    def optimize(self, train_data: Tuple[np.ndarray, np.ndarray], \n",
    "                val_data: Tuple[np.ndarray, np.ndarray], n_trials: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"Run optimization\"\"\"\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        objective = lambda trial: self.objective(trial, train_data, val_data)\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        print(f\"Best score: {study.best_value:.4f}\")\n",
    "        print(\"Best params:\", study.best_params)\n",
    "        \n",
    "        return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Handler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleHandler:\n",
    "    def __init__(self, config: \"Config\"):\n",
    "        self.config = config\n",
    "        self.api = KaggleApi()\n",
    "        self.api.authenticate()\n",
    "    \n",
    "    def upload_pipeline(self, pipeline: Any, dataset_title: Optional[str] = None):\n",
    "        \"\"\"Upload pipeline to Kaggle dataset\"\"\"\n",
    "        if IS_KAGGLE:\n",
    "            raise ValueError(\"This function is for local environment only\")\n",
    "        \n",
    "        tmp_dir = \"./kaggle_upload\"\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{self.config.dataset_name.split('/')[-1]}.pkl\"\n",
    "        pipeline_path = os.path.join(tmp_dir, filename)\n",
    "        \n",
    "        original_path = pipeline.config.model_path\n",
    "        pipeline.config.model_path = os.path.join(os.path.dirname(original_path), filename)\n",
    "        \n",
    "        pipeline.save()\n",
    "        \n",
    "        import shutil\n",
    "        shutil.copy2(pipeline.config.model_path, pipeline_path)\n",
    "        \n",
    "        metadata = {\n",
    "            \"title\": dataset_title or self.config.dataset_name.split('/')[-1],\n",
    "            \"id\": f\"{self.config.dataset_name}\",\n",
    "            \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        with open(os.path.join(tmp_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Creating new dataset: {self.config.dataset_name}\")\n",
    "            self.api.dataset_create_new(\n",
    "                folder=tmp_dir,\n",
    "                public=False, # For private datasets\n",
    "                quiet=False\n",
    "            )\n",
    "            print(\"Dataset created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Kaggle dataset: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            shutil.rmtree(tmp_dir)\n",
    "            \n",
    "    def submit_to_competition(self, submission_file: str, message: str, \n",
    "                            competition: str = 'jane-street-market-prediction') -> None:\n",
    "        \"\"\"Submit predictions to Kaggle competition\n",
    "        \n",
    "        Args:\n",
    "            submission_file: Path to submission file\n",
    "            message: Submission description\n",
    "            competition: Competition name (default: jane-street-market-prediction)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\nSubmitting to competition: {competition}\")\n",
    "            print(f\"Submission file: {submission_file}\")\n",
    "            \n",
    "            # Verify file exists\n",
    "            if not os.path.exists(submission_file):\n",
    "                raise FileNotFoundError(f\"Submission file not found: {submission_file}\")\n",
    "            \n",
    "            # Add dataset reference to message\n",
    "            dataset_ref = f\"{self.config.dataset_name}\"\n",
    "            try:\n",
    "                # Verify dataset exists\n",
    "                self.api.dataset_view(dataset_ref)\n",
    "                message = f\"{message}\\nDataset: {dataset_ref}\"\n",
    "                print(f\"Using dataset: {dataset_ref}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not verify dataset {dataset_ref}: {e}\")\n",
    "            \n",
    "            # Submit to competition\n",
    "            print(f\"Message: {message}\")\n",
    "            self.api.competition_submit(\n",
    "                file_name=submission_file,\n",
    "                message=message,\n",
    "                competition=competition\n",
    "            )\n",
    "            print(\"Submission successful!\")\n",
    "            \n",
    "            # Get latest submission details\n",
    "            submissions = self.api.competition_submissions(competition)\n",
    "            if submissions:\n",
    "                latest = submissions[0]  # Most recent submission\n",
    "                print(f\"\\nLatest submission status:\")\n",
    "                print(f\"Date: {latest.date}\")\n",
    "                print(f\"Description: {latest.description}\")\n",
    "                print(f\"Status: {latest.status}\")\n",
    "                if latest.publicScore:\n",
    "                    print(f\"Public Score: {latest.publicScore}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting to competition: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_competition_leaderboard(self, competition: str = 'jane-street-market-prediction') -> None:\n",
    "        \"\"\"Get competition leaderboard information\n",
    "        \n",
    "        Args:\n",
    "            competition: Competition name (default: jane-street-market-prediction)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\nGetting leaderboard for: {competition}\")\n",
    "            \n",
    "            # Get leaderboard\n",
    "            leaderboard = self.api.competition_leaderboard_view(competition)\n",
    "            \n",
    "            # Display top entries\n",
    "            print(\"\\nTop 10 Leaderboard Entries:\")\n",
    "            for i, entry in enumerate(leaderboard[:10], 1):\n",
    "                print(f\"{i}. Team: {entry.teamName}\")\n",
    "                print(f\"   Score: {entry.score}\")\n",
    "                print(f\"   Entries: {entry.entries}\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting leaderboard: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "class Pipeline:\n",
    "    def __init__(self, config: \"Config\"):\n",
    "        self.config = config\n",
    "        self.data_handler = DataHandler(config)\n",
    "        self.model = self._get_model()\n",
    "        self.kaggle_handler = KaggleHandler(config) if not IS_KAGGLE else None\n",
    "    \n",
    "    def _get_model(self) -> BaseModel:\n",
    "        \"\"\"Get model instance based on config\"\"\"\n",
    "        model_map = {\n",
    "            'lightgbm': LightGBMModel,\n",
    "            'xgboost': XGBoostModel,\n",
    "            'neural_network': NeuralNetworkModel,\n",
    "        }\n",
    "        model_class = model_map.get(self.config.model.name)\n",
    "        if model_class is None:\n",
    "            raise ValueError(f\"Unknown model: {self.config.model.name}\")\n",
    "        return model_class(self.config.model)\n",
    "    \n",
    "    def train(self, preprocessor: Optional[Callable] = None, \n",
    "            feature_generator: Optional[Callable] = None,\n",
    "            optimize: bool = False,\n",
    "            n_trials: int = 100) -> pl.DataFrame:\n",
    "        \"\"\"Train pipeline and return holdout test set\"\"\"\n",
    "        # Load and prepare data\n",
    "        print(\"Loading and preparing data...\")\n",
    "        _, test_df = self.data_handler.load_data()\n",
    "        \n",
    "        # Split data using configured strategy\n",
    "        print(\"Splitting data using configured strategy...\")\n",
    "        train_range, holdout_test_range = self.config.split_strategy.get_holdout_test(self.data_handler.date_ranges)\n",
    "        splits = self.config.split_strategy.split(train_range)\n",
    "        self.config.split_strategy.visualize_splits(train_range)\n",
    "        \n",
    "        best_model = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        # Train and validate on each split\n",
    "        for i, (train_dates, val_dates) in enumerate(splits):\n",
    "            print(f\"\\nTraining fold {i+1}/{len(splits)}\")\n",
    "           \n",
    "            # Prepare data for training and validation\n",
    "            # do we have to hand over preprocessor and feature_generator here? just init it in the class would be better\n",
    "            train_df = self.data_handler.prepare_data(train_dates, preprocessor, feature_generator)\n",
    "            val_df = self.data_handler.prepare_data(val_dates, preprocessor, feature_generator)\n",
    "            \n",
    "            # X, y, w\n",
    "            train_data = self.data_handler.get_feature_data(train_df)\n",
    "            val_data = self.data_handler.get_feature_data(val_df)\n",
    "\n",
    "            # Clear memory\n",
    "            del train_df, val_df\n",
    "            gc.collect()\n",
    "\n",
    "            # Create new model instance for each fold\n",
    "            fold_model = self._get_model()\n",
    "\n",
    "            # Optionally run optimization (only on first fold)\n",
    "            if optimize and i == 0:\n",
    "                print(\"Running hyperparameter optimization...\")\n",
    "                optimizer = OptimizationHandler(self.config, type(fold_model))\n",
    "                best_params = optimizer.optimize(train_data, val_data, n_trials)\n",
    "                self.config.model.params.update(best_params)\n",
    "                # Recreate model with optimized parameters\n",
    "                del fold_model\n",
    "                gc.collect()\n",
    "                fold_model = self._get_model()\n",
    "            \n",
    "            # Train model\n",
    "            print(\"Training model...\")\n",
    "            fold_model.fit(train_data, val_data)\n",
    "            \n",
    "            # Evaluate on validation set using R2\n",
    "            val_X, val_y, val_w = val_data\n",
    "            val_pred = fold_model.predict(val_X)\n",
    "            _, val_score, _ = r2_metric(val_y, val_pred, val_w)\n",
    "            print(f\"Validation R2 score for fold {i+1}: {val_score:.4f}\")\n",
    "            \n",
    "            # Keep track of best model\n",
    "            # if need to transfer learning, we need to keep the best model and continue training\n",
    "            # or if we want to ensemble fold models, we need to keep all models\n",
    "            if val_score > best_score:\n",
    "                best_score = val_score\n",
    "                if best_model is not None:\n",
    "                    del best_model\n",
    "                    gc.collect()\n",
    "                best_model = fold_model\n",
    "            else:\n",
    "                del fold_model\n",
    "                gc.collect()\n",
    "\n",
    "            # Clear fold data\n",
    "            del train_data, val_data, val_X, val_y, val_w, val_pred\n",
    "            gc.collect()\n",
    "        \n",
    "        # Use best model for final predictions\n",
    "        self.model = best_model\n",
    "        print(f\"\\nBest validation score: {best_score:.4f}\")\n",
    "        \n",
    "        # Save pipeline\n",
    "        print(\"\\nSaving pipeline...\")\n",
    "        self.save()\n",
    "\n",
    "        # After finding best model, generate features for holdout test\n",
    "        holdout_test_df = self.data_handler.prepare_data(holdout_test_range, preprocessor, feature_generator) \n",
    "        \n",
    "        # Clear any remaining memory\n",
    "        gc.collect()\n",
    "\n",
    "        return holdout_test_df\n",
    "\n",
    "    def predict(self, X: pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        print(\"Starting prediction...\")\n",
    "        print(f\"Available features: {self.data_handler.features}\")\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "\n",
    "        print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "        print(\"Feature names:\", self.data_handler.features)\n",
    "        print(\"First row of features:\", X[0])  # 또는 holdout_test_X[0]\n",
    "        print(\"Feature matrix shape:\", X.shape)  # 또는 holdout_test_X.shape\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Save pipeline with detailed logging and version tracking\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)\n",
    "        \n",
    "        # Verify model state\n",
    "        if not hasattr(self.model, 'config') or not hasattr(self.model, 'model'):\n",
    "            raise ValueError(\"Model appears to be uninitialized or invalid\")\n",
    "\n",
    "        # Create detailed config dictionary\n",
    "        config_dict = {\n",
    "            'model': {\n",
    "                'name': self.config.model.name,\n",
    "                'params': dict(self.config.model.params),\n",
    "                'custom_metrics': {k: v.__name__ for k, v in self.config.model.custom_metrics.items()}\n",
    "            },\n",
    "            'paths': {\n",
    "                'model_path': self.config.model_path,\n",
    "                'dataset_name': self.config.dataset_name\n",
    "            },\n",
    "            'seed': self.config.seed\n",
    "        }\n",
    "\n",
    "        # Update split strategy section in config_dict\n",
    "        split_strategy_params = {\n",
    "            'test_ratio': self.config.split_strategy.test_ratio\n",
    "        }\n",
    "        \n",
    "        # Add strategy-specific parameters\n",
    "        if isinstance(self.config.split_strategy, TimeBasedSplit):\n",
    "            split_strategy_params['train_ratio'] = self.config.split_strategy.train_ratio\n",
    "        elif isinstance(self.config.split_strategy, TimeSeriesKFold):\n",
    "            split_strategy_params['n_splits'] = self.config.split_strategy.n_splits\n",
    "        elif isinstance(self.config.split_strategy, PurgedGroupTimeSeriesSplit):\n",
    "            split_strategy_params.update({\n",
    "                'n_splits': self.config.split_strategy.n_splits,\n",
    "                'max_train_group_size': self.config.split_strategy.max_train_group_size,\n",
    "                'max_test_group_size': self.config.split_strategy.max_test_group_size,\n",
    "                'group_gap': self.config.split_strategy.group_gap\n",
    "            })\n",
    "        \n",
    "        config_dict['split_strategy'] = {\n",
    "            'type': type(self.config.split_strategy).__name__,\n",
    "            'params': split_strategy_params\n",
    "        }\n",
    "\n",
    "        # Create pipeline metadata with version information\n",
    "        pipeline_data = {\n",
    "            'model': self.model,\n",
    "            'config': config_dict,\n",
    "            'data_handler': {\n",
    "                'preprocessor': self.data_handler.preprocessor,\n",
    "                'preprocessor_version': getattr(self.data_handler.preprocessor, 'version', 'unknown'),\n",
    "                'preprocessor_description': getattr(self.data_handler.preprocessor, 'description', ''),\n",
    "                'feature_generator': self.data_handler.feature_generator,\n",
    "                'feature_generator_version': getattr(self.data_handler.feature_generator, 'version', 'unknown'),\n",
    "                'feature_generator_description': getattr(self.data_handler.feature_generator, 'description', ''),\n",
    "                'features': self.data_handler.features,\n",
    "                'n_features': len(self.data_handler.features)\n",
    "            },\n",
    "            'version': '1.0',\n",
    "            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        # Print pipeline information\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Pipeline Information:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\n1. Model Configuration:\")\n",
    "        print(f\"   - Model Type: {config_dict['model']['name']}\")\n",
    "        print(f\"   - Number of Features: {len(self.data_handler.features)}\")\n",
    "        print(f\"   - Model Parameters:\")\n",
    "        for k, v in config_dict['model']['params'].items():\n",
    "            print(f\"     * {k}: {v}\")\n",
    "        \n",
    "        print(f\"\\n2. Data Processing:\")\n",
    "        print(f\"   - Preprocessor: {self.data_handler.preprocessor.__name__ if self.data_handler.preprocessor else 'None'}\")\n",
    "        print(f\"     * Version: {getattr(self.data_handler.preprocessor, 'version', 'unknown')}\")\n",
    "        print(f\"     * Description: {getattr(self.data_handler.preprocessor, 'description', '')}\")\n",
    "        print(f\"   - Feature Generator: {self.data_handler.feature_generator.__name__ if self.data_handler.feature_generator else 'None'}\")\n",
    "        print(f\"     * Version: {getattr(self.data_handler.feature_generator, 'version', 'unknown')}\")\n",
    "        print(f\"     * Description: {getattr(self.data_handler.feature_generator, 'description', '')}\")\n",
    "        print(f\"   - First 5 Features: {self.data_handler.features[:5]}\")\n",
    "        \n",
    "        print(f\"\\n3. Split Strategy:\")\n",
    "        print(f\"   - Type: {config_dict['split_strategy']['type']}\")\n",
    "        for k, v in config_dict['split_strategy']['params'].items():\n",
    "            print(f\"   - {k}: {v}\")\n",
    "        \n",
    "        print(f\"\\n4. Save Location:\")\n",
    "        print(f\"   - Path: {self.config.model_path}\")\n",
    "        print(f\"   - Dataset Name: {self.config.dataset_name}\")\n",
    "        print(f\"   - Timestamp: {pipeline_data['timestamp']}\")\n",
    "        \n",
    "        try:\n",
    "            with open(self.config.model_path, 'wb') as f:\n",
    "                dill.dump(pipeline_data, f)\n",
    "            print(\"\\nPipeline saved successfully! ✓\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving pipeline: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load(self, path: Optional[str] = None):\n",
    "        \"\"\"Load pipeline with extensive validation and version tracking\"\"\"\n",
    "        load_path = path if path is not None else self.config.model_path\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Loading Pipeline:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            with open(load_path, 'rb') as f:\n",
    "                pipeline_data = dill.load(f)\n",
    "            \n",
    "            # Phase 1: Structure Validation\n",
    "            print(\"\\nPhase 1: Structure Validation\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            required_components = ['model', 'config', 'data_handler', 'version']\n",
    "            missing = [comp for comp in required_components if comp not in pipeline_data]\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing required components in pipeline: {missing}\")\n",
    "            print(\"✓ Basic structure validation passed\")\n",
    "            \n",
    "            required_data_handler = ['preprocessor', 'feature_generator', 'features', 'n_features',\n",
    "                                'preprocessor_version', 'feature_generator_version']\n",
    "            missing_dh = [comp for comp in required_data_handler if comp not in pipeline_data['data_handler']]\n",
    "            if missing_dh:\n",
    "                raise ValueError(f\"Missing data handler components: {missing_dh}\")\n",
    "            print(\"✓ Data handler structure validation passed\")\n",
    "            \n",
    "            # Phase 2: Model and Config Reconstruction\n",
    "            print(\"\\nPhase 2: Model and Config Reconstruction\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            self.model = pipeline_data['model']\n",
    "            config_dict = pipeline_data['config']\n",
    "            \n",
    "            # Reconstruct split strategy\n",
    "            split_strategy_type = config_dict['split_strategy']['type']\n",
    "            split_params = config_dict['split_strategy']['params']\n",
    "            \n",
    "            if split_strategy_type == 'TimeBasedSplit':\n",
    "                self.config.split_strategy = TimeBasedSplit(\n",
    "                    train_ratio=split_params.get('train_ratio', 0.75),\n",
    "                    test_ratio=split_params.get('test_ratio', 0.2)\n",
    "                )\n",
    "            elif split_strategy_type == 'TimeSeriesKFold':\n",
    "                self.config.split_strategy = TimeSeriesKFold(\n",
    "                    n_splits=split_params.get('n_splits', 5),\n",
    "                    test_ratio=split_params.get('test_ratio', 0.2)\n",
    "                )\n",
    "            elif split_strategy_type == 'PurgedGroupTimeSeriesSplit':\n",
    "                self.config.split_strategy = PurgedGroupTimeSeriesSplit(\n",
    "                    n_splits=split_params.get('n_splits', 5),\n",
    "                    max_train_group_size=split_params.get('max_train_group_size', np.inf),\n",
    "                    max_test_group_size=split_params.get('max_test_group_size', np.inf),\n",
    "                    group_gap=split_params.get('group_gap', None),\n",
    "                    test_ratio=split_params.get('test_ratio', 0.2)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown split strategy type: {split_strategy_type}\")\n",
    "            \n",
    "            print(\"✓ Split strategy reconstruction passed\")\n",
    "            \n",
    "            # Model validation\n",
    "            required_model_attrs = ['predict', 'model', 'config']\n",
    "            missing_attrs = [attr for attr in required_model_attrs if not hasattr(self.model, attr)]\n",
    "            if missing_attrs:\n",
    "                raise AttributeError(f\"Model missing required attributes: {missing_attrs}\")\n",
    "            print(\"✓ Model attributes validation passed\")\n",
    "            \n",
    "            # Test model with dummy data\n",
    "            try:\n",
    "                n_features = len(pipeline_data['data_handler']['features'])\n",
    "                dummy_input = np.random.random((5, n_features))\n",
    "                dummy_pred = self.model.predict(dummy_input)\n",
    "                if not isinstance(dummy_pred, np.ndarray):\n",
    "                    raise TypeError(f\"Model prediction returned {type(dummy_pred)}, expected numpy.ndarray\")\n",
    "                if len(dummy_pred.shape) != 1 or len(dummy_pred) != 5:\n",
    "                    raise ValueError(f\"Unexpected prediction shape: {dummy_pred.shape}, expected (5,)\")\n",
    "                print(\"✓ Model prediction test passed\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Model prediction test failed: {str(e)}\")\n",
    "            \n",
    "            # Phase 3: Function Version Validation\n",
    "            print(\"\\nPhase 3: Function Version Validation\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            self.data_handler.preprocessor = pipeline_data['data_handler']['preprocessor']\n",
    "            self.data_handler.feature_generator = pipeline_data['data_handler']['feature_generator']\n",
    "            self.data_handler.features = pipeline_data['data_handler']['features']\n",
    "            \n",
    "            # Version validation\n",
    "            preprocessor_version = getattr(self.data_handler.preprocessor, 'version', 'unknown')\n",
    "            saved_preprocessor_version = pipeline_data['data_handler']['preprocessor_version']\n",
    "            if preprocessor_version != saved_preprocessor_version:\n",
    "                print(f\"⚠️ Warning: Current preprocessor version ({preprocessor_version}) \"\n",
    "                    f\"differs from saved version ({saved_preprocessor_version})\")\n",
    "            \n",
    "            feature_gen_version = getattr(self.data_handler.feature_generator, 'version', 'unknown')\n",
    "            saved_feature_gen_version = pipeline_data['data_handler']['feature_generator_version']\n",
    "            if feature_gen_version != saved_feature_gen_version:\n",
    "                print(f\"⚠️ Warning: Current feature generator version ({feature_gen_version}) \"\n",
    "                    f\"differs from saved version ({saved_feature_gen_version})\")\n",
    "            \n",
    "            # Phase 4: Data Handler Function Validation\n",
    "            print(\"\\nPhase 4: Data Handler Function Validation\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Validate preprocessor\n",
    "            if self.data_handler.preprocessor:\n",
    "                try:\n",
    "                    dummy_df = pl.DataFrame({\n",
    "                        'time_id': np.arange(5),\n",
    "                        'symbol_id': np.ones(5),\n",
    "                        'weight': np.ones(5),\n",
    "                        **{f'feature_{i:02d}': np.random.random(5) for i in range(79)}\n",
    "                    })\n",
    "                    processed_df = self.data_handler.preprocessor(dummy_df)\n",
    "                    if not isinstance(processed_df, pl.DataFrame):\n",
    "                        raise TypeError(f\"Preprocessor returned {type(processed_df)}, expected polars.DataFrame\")\n",
    "                    print(\"✓ Preprocessor function test passed\")\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Preprocessor function test failed: {str(e)}\")\n",
    "            \n",
    "            # Validate feature generator\n",
    "            if self.data_handler.feature_generator:\n",
    "                try:\n",
    "                    dummy_df = pl.DataFrame({\n",
    "                        'time_id': np.arange(5),\n",
    "                        'symbol_id': np.ones(5),\n",
    "                        'weight': np.ones(5),\n",
    "                        **{f'feature_{i:02d}': np.random.random(5) for i in range(79)}\n",
    "                    })\n",
    "                    generated_df = self.data_handler.feature_generator(dummy_df)\n",
    "                    if not isinstance(generated_df, pl.DataFrame):\n",
    "                        raise TypeError(f\"Feature generator returned {type(generated_df)}, expected polars.DataFrame\")\n",
    "                    print(\"✓ Feature generator function test passed\")\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Feature generator function test failed: {str(e)}\")\n",
    "            \n",
    "            # Validate features list\n",
    "            if not self.data_handler.features:\n",
    "                raise ValueError(\"Features list is empty\")\n",
    "            if not all(isinstance(f, str) for f in self.data_handler.features):\n",
    "                raise TypeError(\"All feature names must be strings\")\n",
    "            if len(self.data_handler.features) != len(set(self.data_handler.features)):\n",
    "                raise ValueError(\"Duplicate feature names found\")\n",
    "            print(\"✓ Features list validation passed\")\n",
    "            \n",
    "            # Phase 5: Pipeline Information\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"Pipeline Information:\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            print(f\"\\n1. Model Configuration:\")\n",
    "            print(f\"   - Model Type: {config_dict['model']['name']}\")\n",
    "            print(f\"   - Number of Features: {pipeline_data['data_handler']['n_features']}\")\n",
    "            print(f\"   - Model Parameters:\")\n",
    "            for k, v in config_dict['model']['params'].items():\n",
    "                print(f\"     * {k}: {v}\")\n",
    "            \n",
    "            print(f\"\\n2. Data Processing:\")\n",
    "            print(f\"   - Preprocessor: {self.data_handler.preprocessor.__name__ if self.data_handler.preprocessor else 'None'}\")\n",
    "            print(f\"     * Version: {saved_preprocessor_version}\")\n",
    "            print(f\"     * Description: {pipeline_data['data_handler']['preprocessor_description']}\")\n",
    "            print(f\"   - Feature Generator: {self.data_handler.feature_generator.__name__ if self.data_handler.feature_generator else 'None'}\")\n",
    "            print(f\"     * Version: {saved_feature_gen_version}\")\n",
    "            print(f\"     * Description: {pipeline_data['data_handler']['feature_generator_description']}\")\n",
    "            print(f\"   - First 5 Features: {self.data_handler.features[:5]}\")\n",
    "            \n",
    "            print(f\"\\n3. Split Strategy:\")\n",
    "            print(f\"   - Type: {config_dict['split_strategy']['type']}\")\n",
    "            for k, v in config_dict['split_strategy']['params'].items():\n",
    "                print(f\"   - {k}: {v}\")\n",
    "            \n",
    "            print(f\"\\n4. Load Location:\")\n",
    "            print(f\"   - Path: {load_path}\")\n",
    "            print(f\"   - Dataset Name: {config_dict['paths']['dataset_name']}\")\n",
    "            print(f\"   - Original Save Timestamp: {pipeline_data['timestamp']}\")\n",
    "            \n",
    "            # Final validation: Complete pipeline test\n",
    "            try:\n",
    "                dummy_features = {f: np.random.random(5) for f in self.data_handler.features}\n",
    "                dummy_df = pl.DataFrame({\n",
    "                    **dummy_features,\n",
    "                    'weight': np.ones(5)\n",
    "                })\n",
    "                X, _, _ = self.data_handler.get_feature_data(dummy_df)\n",
    "                final_pred = self.model.predict(X)\n",
    "                print(\"\\n✓ Complete pipeline test passed\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Complete pipeline test failed: {str(e)}\")\n",
    "            \n",
    "            print(\"\\nPipeline loaded and validated successfully! ✓\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError loading pipeline: {str(e)}\")\n",
    "            raise\n",
    "    def upload_to_kaggle(self, dataset_title: Optional[str] = None):\n",
    "        \"\"\"Upload this pipeline to Kaggle dataset\"\"\"\n",
    "        self.kaggle_handler.upload_pipeline(self, dataset_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str = 'lightgbm'\n",
    "    params: Dict[str, Any] = None\n",
    "    custom_metrics: Dict[str, Callable] = field(default_factory=dict)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.params is None:\n",
    "            self.params = self.get_default_params()\n",
    "    \n",
    "    def get_default_params(self) -> Dict[str, Any]:\n",
    "        params = {\n",
    "            'lightgbm': {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'verbosity': -1,\n",
    "                'boosting_type': 'gbdt',\n",
    "                'learning_rate': 0.05,\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'eval_metric': 'rmse',\n",
    "                'verbosity': 0,\n",
    "            },\n",
    "            'neural_network': {\n",
    "                'learning_rate': 0.001,\n",
    "                'batch_size': 512,\n",
    "                'epochs': 10,\n",
    "            }\n",
    "        }\n",
    "        return params.get(self.name, {})\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model\n",
    "    model: ModelConfig = ModelConfig()\n",
    "    # Paths\n",
    "    model_path: str = f\"{MODEL_PATH}/pipeline.pkl\"\n",
    "    dataset_name: str = \"jane-street-model\"\n",
    "    # Data loading\n",
    "    partition_range: Optional[List[int]] = None\n",
    "    # Training\n",
    "    split_strategy: SplitStrategy = field(default_factory=lambda: TimeBasedSplit(train_ratio=0.75, test_ratio=0.2))\n",
    "    seed: int = 42\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        np.random.seed(self.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Optimize data types for memory usage in Polars\"\"\"\n",
    "    start_mem = df.estimated_size() / (1024**2)\n",
    "    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type in [pl.Float64, pl.Float32, pl.Int64, pl.Int32, pl.Int16, pl.Int8]:\n",
    "            c_min = df[col].drop_nulls().min()\n",
    "            c_max = df[col].drop_nulls().max()\n",
    "            \n",
    "            if c_min is not None and c_max is not None:  # null check 추가\n",
    "                if col_type in [pl.Int64, pl.Int32, pl.Int16, pl.Int8]:\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df = df.with_columns(pl.col(col).cast(pl.Int8))\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df = df.with_columns(pl.col(col).cast(pl.Int16))\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "                    else:\n",
    "                        df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "        \n",
    "        elif col_type == pl.Utf8:\n",
    "            df = df.with_columns(pl.col(col).cast(pl.Categorical))\n",
    "    \n",
    "    end_mem = df.estimated_size() / (1024**2)\n",
    "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@versioned_function(\"1.0.0\", \"Initial preprocessor with basic cleaning\")\n",
    "def default_preprocessor(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Default preprocessing function\"\"\"\n",
    "    df = reduce_memory(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "@versioned_function(\"1.1.0\", \"Added time-based features and symbol_id processing\")\n",
    "def default_feature_generator(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Feature generation with time-based features\"\"\"\n",
    "    # Add time-based features using polars expressions\n",
    "    result = df.with_columns([\n",
    "        (2 * np.pi * pl.col('time_id') / 967).sin().alias('feature_sin_time_id'),\n",
    "        (2 * np.pi * pl.col('time_id') / 967).cos().alias('feature_cos_time_id'),\n",
    "        (2 * np.pi * pl.col('time_id') / 483).sin().alias('feature_sin_time_id_halfday'),\n",
    "        (2 * np.pi * pl.col('time_id') / 483).cos().alias('feature_cos_time_id_halfday')\n",
    "    ])\n",
    "\n",
    "    # Fill NA values and rename columns\n",
    "    result = (result\n",
    "    .fill_null(-1)\n",
    "    .rename({\n",
    "        'symbol_id': 'feature_symbol_id',\n",
    "        'weight': 'feature_weight'\n",
    "    }))\n",
    "\n",
    "    # Select and reorder columns\n",
    "    feature_cols = ['feature_symbol_id', 'feature_sin_time_id', 'feature_cos_time_id',\n",
    "                    'feature_sin_time_id_halfday', 'feature_cos_time_id_halfday', 'feature_weight']\n",
    "    feature_cols.extend([f'feature_0{i}' if i < 10 else f'feature_{i}'\n",
    "                         for i in range(79)])\n",
    "\n",
    "    # Add target column if it exists\n",
    "    if 'responder_6' in result.columns:\n",
    "        feature_cols.insert(0, 'responder_6')\n",
    "\n",
    "    return result.select(feature_cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_: pl.DataFrame | None = None\n",
    "historical_feature_buffer: pl.DataFrame | None = None\n",
    "historical_feature_update = 0 \n",
    "lags_buffer: pl.DataFrame | None = None \n",
    "lags_update = 0\n",
    "\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame:\n",
    "    \"\"\"Competition prediction function\"\"\"\n",
    "    global pipeline\n",
    "    global lags_\n",
    "    global historical_feature_buffer, historical_feature_update\n",
    "    global lags_buffer, lags_update\n",
    "    \n",
    "    row_ids = test['row_id'].to_numpy()\n",
    "\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "        if historical_feature_update >= N: # lags data가 n번 쌓였을 때 학습 진행\n",
    "            ol_data = pl.concat([historical_feature_buffer, lags_buffer], how='horizontal') # Online Learning에 사용될 데이터를 생성            \n",
    "            pipeline.train(ol_data)  # <- 데이터를 지정해서 학습할 수 있는 방법을 pipeline에 도입해야함. \n",
    "            historical_feature_buffer = None # 학습 이후에 buffer init\n",
    "            lags_buffer = None # 학습 이후에 buffer init\n",
    "            historical_feature_update = 0 # 학습 이후에 update init\n",
    "            lags_buffer_update = 0 # 학습 이후에 update init\n",
    "            \n",
    "        \n",
    "    if pipeline.data_handler.preprocessor:\n",
    "        test = pipeline.data_handler.preprocessor(test)\n",
    "    if pipeline.data_handler.feature_generator:\n",
    "        test = pipeline.data_handler.feature_generator(test)\n",
    "\n",
    "\n",
    "    test_X, _, _ = pipeline.data_handler.get_feature_data(test)\n",
    "    \n",
    "    # 이부분에서 global로 선언된 historical_feature_buffer에 test를 historical_feature_buffer에 저장함.\n",
    "    if lags is not None:\n",
    "        if historical_feature_buffer is None:\n",
    "            historical_feature_buffer = test_X\n",
    "            historical_feature_update += 1 \n",
    "        else:\n",
    "            # concat을 사용해 새로운 행을 추가\n",
    "            historical_feature_buffer = pl.concat([historical_feature_buffer, test_X], how=\"vertical\") \n",
    "            historical_feature_update += 1 \n",
    "            \n",
    "        # 만약 lags != None -> lags를 lags_buffer에 저장 -> [lags_buffer & historical_feature_buffer]를 통해서 학습\n",
    "        if lags_buffer is None:\n",
    "            lags_buffer = lags\n",
    "            lags_update += 1 \n",
    "        else:\n",
    "            # concat을 사용해 새로운 행을 추가\n",
    "            lags_buffer = pl.concat([lags_buffer, lags], how=\"vertical\") \n",
    "            lags_update += 1 \n",
    "\n",
    "\n",
    "    \n",
    "    predictions = pipeline.predict(test_X)\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "\n",
    "    result = pl.DataFrame({\n",
    "        'row_id': row_ids,\n",
    "        'responder_6': predictions\n",
    "    })\n",
    "    \n",
    "    # Validation checks\n",
    "    assert isinstance(result, (pl.DataFrame, pd.DataFrame))\n",
    "    assert result.columns == ['row_id', 'responder_6']\n",
    "    assert len(result) == len(test)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def run_inference_only(dataset_name: str, model_filename: str = 'pipeline.pkl') -> Pipeline:\n",
    "    \"\"\"Kaggle dataset에서 모델 로드하고 inference 준비\"\"\"\n",
    "    if not IS_KAGGLE:\n",
    "        raise ValueError(\"This function is for Kaggle environment only\")\n",
    "    \n",
    "    # Kaggle dataset에서 모델 파일 경로\n",
    "    model_path = f'/kaggle/input/{dataset_name}/{dataset_name}.pkl'\n",
    "    \n",
    "    # 파이프라인 초기화 및 모델 로드\n",
    "    pipeline = Pipeline(Config())\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    pipeline.load(model_path)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model locally...\n",
      "Loading and preparing data...\n",
      "\n",
      "Train Data Date Ranges per Partition:\n",
      "Partition 0: date_id 0 to 169\n",
      "Partition 1: date_id 170 to 339\n",
      "Partition 2: date_id 340 to 509\n",
      "Partition 3: date_id 510 to 679\n",
      "Partition 4: date_id 680 to 849\n",
      "Partition 5: date_id 850 to 1019\n",
      "Partition 6: date_id 1020 to 1189\n",
      "Partition 7: date_id 1190 to 1359\n",
      "Partition 8: date_id 1360 to 1529\n",
      "Partition 9: date_id 1530 to 1698\n",
      "Splitting data using configured strategy...\n",
      "\n",
      "Holdout Test Split Info:\n",
      "Total unique dates: 1699\n",
      "Train dates range: 0 - 1359\n",
      "Test dates range: 1360 - 1698\n",
      "\n",
      "Purged Group Time Series 5-Fold Split Info:\n",
      "Total days: 1360\n",
      "Group test size: 200\n",
      "Group gap: 50\n",
      "\n",
      "Fold:\n",
      "Train dates range: 0 - 310\n",
      "Gap dates range: 311 - 409\n",
      "Test dates range: 410 - 560\n",
      "\n",
      "Fold:\n",
      "Train dates range: 110 - 510\n",
      "Gap dates range: 511 - 609\n",
      "Test dates range: 610 - 760\n",
      "\n",
      "Fold:\n",
      "Train dates range: 310 - 710\n",
      "Gap dates range: 711 - 809\n",
      "Test dates range: 810 - 960\n",
      "\n",
      "Fold:\n",
      "Train dates range: 510 - 910\n",
      "Gap dates range: 911 - 1009\n",
      "Test dates range: 1010 - 1160\n",
      "\n",
      "Fold:\n",
      "Train dates range: 710 - 1110\n",
      "Gap dates range: 1111 - 1209\n",
      "Test dates range: 1210 - 1359\n",
      "\n",
      "Purged Group Time Series 5-Fold Split Info:\n",
      "Total days: 1360\n",
      "Group test size: 200\n",
      "Group gap: 50\n",
      "\n",
      "Fold:\n",
      "Train dates range: 0 - 310\n",
      "Gap dates range: 311 - 409\n",
      "Test dates range: 410 - 560\n",
      "\n",
      "Fold:\n",
      "Train dates range: 110 - 510\n",
      "Gap dates range: 511 - 609\n",
      "Test dates range: 610 - 760\n",
      "\n",
      "Fold:\n",
      "Train dates range: 310 - 710\n",
      "Gap dates range: 711 - 809\n",
      "Test dates range: 810 - 960\n",
      "\n",
      "Fold:\n",
      "Train dates range: 510 - 910\n",
      "Gap dates range: 911 - 1009\n",
      "Test dates range: 1010 - 1160\n",
      "\n",
      "Fold:\n",
      "Train dates range: 710 - 1110\n",
      "Gap dates range: 1111 - 1209\n",
      "Test dates range: 1210 - 1359\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1/5\n",
      "\n",
      "Loading train data from partitions: [0, 1]\n",
      "Loaded train data shape: (4256886, 92)\n",
      "Date range in loaded data: 0 to 310\n",
      "Memory usage of dataframe is 1452.54 MB\n",
      "Memory usage after optimization is: 1452.54 MB\n",
      "Decreased by 0.0%\n",
      "\n",
      "Loading train data from partitions: [2, 3]\n",
      "Loaded train data shape: (3086115, 92)\n",
      "Date range in loaded data: 410 to 560\n",
      "Memory usage of dataframe is 1050.64 MB\n",
      "Memory usage after optimization is: 1050.64 MB\n",
      "Decreased by 0.0%\n",
      "Training model...\n",
      "\n",
      "Using device: cuda\n",
      "Number of training batches: 4158\n",
      "Number of validation batches: 3014\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/100: train_loss = 0.7340, val_loss = 0.9471, best_val_loss = 0.9471\n",
      "Epoch 2/100: train_loss = 0.7260, val_loss = 0.9529, best_val_loss = 0.9471\n",
      "Epoch 3/100: train_loss = 0.7214, val_loss = 0.9626, best_val_loss = 0.9471\n",
      "Epoch 4/100: train_loss = 0.7172, val_loss = 0.9520, best_val_loss = 0.9471\n",
      "Epoch 5/100: train_loss = 0.7137, val_loss = 1.0200, best_val_loss = 0.9471\n",
      "Epoch 6/100: train_loss = 0.7093, val_loss = 0.9903, best_val_loss = 0.9471\n",
      "Epoch 7/100: train_loss = 0.7063, val_loss = 1.0120, best_val_loss = 0.9471\n",
      "Epoch 8/100: train_loss = 0.7033, val_loss = 0.9753, best_val_loss = 0.9471\n",
      "Epoch 9/100: train_loss = 0.7008, val_loss = 0.9738, best_val_loss = 0.9471\n",
      "Epoch 10/100: train_loss = 0.6982, val_loss = 0.9625, best_val_loss = 0.9471\n",
      "Epoch 11/100: train_loss = 0.6957, val_loss = 1.0313, best_val_loss = 0.9471\n",
      "\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Training completed!\n",
      "X shape: (3086115, 85)\n",
      "X type: <class 'numpy.ndarray'>\n",
      "After conversion - X shape: (3086115, 85)\n",
      "After conversion - X type: <class 'numpy.ndarray'>\n",
      "After conversion - X dtype: float32\n",
      "Validation R2 score for fold 1: -0.0848\n",
      "\n",
      "Training fold 2/5\n",
      "\n",
      "Loading train data from partitions: [0, 1, 2, 3]\n",
      "Loaded train data shape: (6690120, 92)\n",
      "Date range in loaded data: 110 to 510\n",
      "Memory usage of dataframe is 2280.10 MB\n",
      "Memory usage after optimization is: 2280.10 MB\n",
      "Decreased by 0.0%\n",
      "\n",
      "Loading train data from partitions: [3, 4]\n",
      "Loaded train data shape: (3978137, 92)\n",
      "Date range in loaded data: 610 to 760\n",
      "Memory usage of dataframe is 1352.84 MB\n",
      "Memory usage after optimization is: 1352.84 MB\n",
      "Decreased by 0.0%\n",
      "Training model...\n",
      "\n",
      "Using device: cuda\n",
      "Number of training batches: 6534\n",
      "Number of validation batches: 3885\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/100: train_loss = 0.8442, val_loss = 4.0832, best_val_loss = 4.0832\n",
      "Epoch 2/100: train_loss = 0.8361, val_loss = 4.6811, best_val_loss = 4.0832\n",
      "Epoch 3/100: train_loss = 0.8318, val_loss = 1.3674, best_val_loss = 1.3674\n",
      "Epoch 4/100: train_loss = 0.8285, val_loss = 2.5698, best_val_loss = 1.3674\n",
      "Epoch 5/100: train_loss = 0.8256, val_loss = 0.9760, best_val_loss = 0.9760\n",
      "Epoch 6/100: train_loss = 0.8231, val_loss = 0.9368, best_val_loss = 0.9368\n",
      "Epoch 7/100: train_loss = 0.8207, val_loss = 0.8907, best_val_loss = 0.8907\n",
      "Epoch 8/100: train_loss = 0.8181, val_loss = 0.8893, best_val_loss = 0.8893\n",
      "Epoch 9/100: train_loss = 0.8164, val_loss = 0.9394, best_val_loss = 0.8893\n",
      "Epoch 10/100: train_loss = 0.8142, val_loss = 0.9510, best_val_loss = 0.8893\n",
      "Epoch 11/100: train_loss = 0.8127, val_loss = 13.2677, best_val_loss = 0.8893\n",
      "Epoch 12/100: train_loss = 0.8111, val_loss = 0.9523, best_val_loss = 0.8893\n",
      "Epoch 13/100: train_loss = 0.8098, val_loss = 0.9737, best_val_loss = 0.8893\n",
      "Epoch 14/100: train_loss = 0.8085, val_loss = 0.9888, best_val_loss = 0.8893\n",
      "Epoch 15/100: train_loss = 0.8075, val_loss = 0.9361, best_val_loss = 0.8893\n",
      "Epoch 16/100: train_loss = 0.8065, val_loss = 1.1250, best_val_loss = 0.8893\n",
      "Epoch 17/100: train_loss = 0.8055, val_loss = 0.9444, best_val_loss = 0.8893\n",
      "Epoch 18/100: train_loss = 0.8048, val_loss = 1.1745, best_val_loss = 0.8893\n",
      "\n",
      "Early stopping at epoch 18\n",
      "\n",
      "Training completed!\n",
      "X shape: (3978137, 85)\n",
      "X type: <class 'numpy.ndarray'>\n",
      "After conversion - X shape: (3978137, 85)\n",
      "After conversion - X type: <class 'numpy.ndarray'>\n",
      "After conversion - X dtype: float32\n",
      "Validation R2 score for fold 2: -0.3817\n",
      "\n",
      "Training fold 3/5\n",
      "\n",
      "Loading train data from partitions: [1, 2, 3, 4]\n",
      "Loaded train data shape: (8429655, 92)\n",
      "Date range in loaded data: 310 to 710\n",
      "Memory usage of dataframe is 2869.06 MB\n",
      "Memory usage after optimization is: 2869.06 MB\n",
      "Decreased by 0.0%\n",
      "\n",
      "Loading train data from partitions: [4, 5]\n",
      "Loaded train data shape: (4600904, 92)\n",
      "Date range in loaded data: 810 to 960\n",
      "Memory usage of dataframe is 1562.65 MB\n",
      "Memory usage after optimization is: 1562.65 MB\n",
      "Decreased by 0.0%\n",
      "Training model...\n",
      "\n",
      "Using device: cuda\n",
      "Number of training batches: 8233\n",
      "Number of validation batches: 4494\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/100: train_loss = 0.9434, val_loss = 0.7893, best_val_loss = 0.7893\n",
      "Epoch 2/100: train_loss = 0.9351, val_loss = 0.7948, best_val_loss = 0.7893\n",
      "Epoch 3/100: train_loss = 0.9311, val_loss = 0.7827, best_val_loss = 0.7827\n",
      "Epoch 4/100: train_loss = 0.9276, val_loss = 0.7833, best_val_loss = 0.7827\n",
      "Epoch 5/100: train_loss = 0.9248, val_loss = 0.7915, best_val_loss = 0.7827\n",
      "Epoch 6/100: train_loss = 0.9222, val_loss = 1.2989, best_val_loss = 0.7827\n",
      "Epoch 7/100: train_loss = 0.9201, val_loss = 0.7845, best_val_loss = 0.7827\n",
      "Epoch 8/100: train_loss = 0.9179, val_loss = 0.7805, best_val_loss = 0.7805\n",
      "Epoch 9/100: train_loss = 0.9161, val_loss = 0.7989, best_val_loss = 0.7805\n",
      "Epoch 10/100: train_loss = 0.9145, val_loss = 0.7921, best_val_loss = 0.7805\n",
      "Epoch 11/100: train_loss = 0.9129, val_loss = 0.7855, best_val_loss = 0.7805\n",
      "Epoch 12/100: train_loss = 0.9118, val_loss = 0.7936, best_val_loss = 0.7805\n",
      "Epoch 13/100: train_loss = 0.9105, val_loss = 0.7843, best_val_loss = 0.7805\n",
      "Epoch 14/100: train_loss = 0.9092, val_loss = 0.7982, best_val_loss = 0.7805\n",
      "Epoch 15/100: train_loss = 0.9083, val_loss = 0.7849, best_val_loss = 0.7805\n",
      "Epoch 16/100: train_loss = 0.9070, val_loss = 0.7877, best_val_loss = 0.7805\n",
      "Epoch 17/100: train_loss = 0.9062, val_loss = 0.7879, best_val_loss = 0.7805\n",
      "Epoch 18/100: train_loss = 0.9056, val_loss = 0.7934, best_val_loss = 0.7805\n",
      "\n",
      "Early stopping at epoch 18\n",
      "\n",
      "Training completed!\n",
      "X shape: (4600904, 85)\n",
      "X type: <class 'numpy.ndarray'>\n",
      "After conversion - X shape: (4600904, 85)\n",
      "After conversion - X type: <class 'numpy.ndarray'>\n",
      "After conversion - X dtype: float32\n",
      "Validation R2 score for fold 3: -0.0358\n",
      "\n",
      "Training fold 4/5\n",
      "\n",
      "Loading train data from partitions: [3, 4, 5]\n",
      "Loaded train data shape: (10884744, 92)\n",
      "Date range in loaded data: 510 to 910\n",
      "Memory usage of dataframe is 3700.06 MB\n",
      "Memory usage after optimization is: 3700.06 MB\n",
      "Decreased by 0.0%\n",
      "\n",
      "Loading train data from partitions: [5, 6]\n",
      "Loaded train data shape: (5453712, 92)\n",
      "Date range in loaded data: 1010 to 1160\n",
      "Memory usage of dataframe is 1850.80 MB\n",
      "Memory usage after optimization is: 1850.80 MB\n",
      "Decreased by 0.0%\n",
      "Training model...\n",
      "\n",
      "Using device: cuda\n",
      "Number of training batches: 10630\n",
      "Number of validation batches: 5326\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/100: train_loss = 0.9153, val_loss = 0.7004, best_val_loss = 0.7004\n",
      "Epoch 2/100: train_loss = 0.9079, val_loss = 0.7005, best_val_loss = 0.7004\n",
      "Epoch 3/100: train_loss = 0.9038, val_loss = 0.8772, best_val_loss = 0.7004\n",
      "Epoch 4/100: train_loss = 0.9005, val_loss = 0.7063, best_val_loss = 0.7004\n",
      "Epoch 5/100: train_loss = 0.8979, val_loss = 0.7542, best_val_loss = 0.7004\n",
      "Epoch 6/100: train_loss = 0.8953, val_loss = 0.7925, best_val_loss = 0.7004\n",
      "Epoch 7/100: train_loss = 0.8937, val_loss = 0.7009, best_val_loss = 0.7004\n",
      "Epoch 8/100: train_loss = 0.8921, val_loss = 0.7199, best_val_loss = 0.7004\n",
      "Epoch 9/100: train_loss = 0.8905, val_loss = 0.7024, best_val_loss = 0.7004\n",
      "Epoch 10/100: train_loss = 0.8891, val_loss = 0.7015, best_val_loss = 0.7004\n",
      "Epoch 11/100: train_loss = 0.8878, val_loss = 0.7011, best_val_loss = 0.7004\n",
      "\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Training completed!\n",
      "X shape: (5453712, 85)\n",
      "X type: <class 'numpy.ndarray'>\n",
      "After conversion - X shape: (5453712, 85)\n",
      "After conversion - X type: <class 'numpy.ndarray'>\n",
      "After conversion - X dtype: float32\n",
      "Validation R2 score for fold 4: 0.0014\n",
      "\n",
      "Training fold 5/5\n",
      "\n",
      "Loading train data from partitions: [4, 5, 6]\n",
      "Loaded train data shape: (12758240, 92)\n",
      "Date range in loaded data: 710 to 1110\n",
      "Memory usage of dataframe is 4333.17 MB\n",
      "Memory usage after optimization is: 4333.17 MB\n",
      "Decreased by 0.0%\n",
      "\n",
      "Loading train data from partitions: [7]\n",
      "Loaded train data shape: (5600848, 92)\n",
      "Date range in loaded data: 1210 to 1359\n",
      "Memory usage of dataframe is 1901.40 MB\n",
      "Memory usage after optimization is: 1901.40 MB\n",
      "Decreased by 0.0%\n",
      "Training model...\n",
      "\n",
      "Using device: cuda\n",
      "Number of training batches: 12460\n",
      "Number of validation batches: 5470\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/100: train_loss = 0.7837, val_loss = 0.6128, best_val_loss = 0.6128\n",
      "Epoch 2/100: train_loss = 0.7773, val_loss = 0.6139, best_val_loss = 0.6128\n",
      "Epoch 3/100: train_loss = 0.7747, val_loss = 0.6149, best_val_loss = 0.6128\n",
      "Epoch 4/100: train_loss = 0.7727, val_loss = 0.6610, best_val_loss = 0.6128\n",
      "Epoch 5/100: train_loss = 0.7710, val_loss = 0.6199, best_val_loss = 0.6128\n",
      "Epoch 6/100: train_loss = 0.7694, val_loss = 0.6170, best_val_loss = 0.6128\n",
      "Epoch 7/100: train_loss = 0.7682, val_loss = 0.6193, best_val_loss = 0.6128\n",
      "Epoch 8/100: train_loss = 0.7667, val_loss = 0.6497, best_val_loss = 0.6128\n",
      "Epoch 9/100: train_loss = 0.7658, val_loss = 0.6192, best_val_loss = 0.6128\n",
      "Epoch 10/100: train_loss = 0.7650, val_loss = 0.6368, best_val_loss = 0.6128\n",
      "Epoch 11/100: train_loss = 0.7641, val_loss = 0.6343, best_val_loss = 0.6128\n",
      "\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Training completed!\n",
      "X shape: (5600848, 85)\n",
      "X type: <class 'numpy.ndarray'>\n",
      "After conversion - X shape: (5600848, 85)\n",
      "After conversion - X type: <class 'numpy.ndarray'>\n",
      "After conversion - X dtype: float32\n",
      "Validation R2 score for fold 5: -0.0291\n",
      "\n",
      "Best validation score: 0.0014\n",
      "\n",
      "Saving pipeline...\n",
      "\n",
      "==================================================\n",
      "Pipeline Information:\n",
      "==================================================\n",
      "\n",
      "1. Model Configuration:\n",
      "   - Model Type: neural_network\n",
      "   - Number of Features: 85\n",
      "   - Model Parameters:\n",
      "     * hidden_dims: [256, 128, 64]\n",
      "     * dropout: 0.1\n",
      "     * learning_rate: 0.001\n",
      "     * weight_decay: 1e-05\n",
      "     * batch_size: 1024\n",
      "     * epochs: 100\n",
      "     * patience: 10\n",
      "\n",
      "2. Data Processing:\n",
      "   - Preprocessor: default_preprocessor\n",
      "     * Version: 1.0.0\n",
      "     * Description: Initial preprocessor with basic cleaning\n",
      "   - Feature Generator: default_feature_generator\n",
      "     * Version: 1.1.0\n",
      "     * Description: Added time-based features and symbol_id processing\n",
      "   - First 5 Features: ['feature_symbol_id', 'feature_sin_time_id', 'feature_cos_time_id', 'feature_sin_time_id_halfday', 'feature_cos_time_id_halfday']\n",
      "\n",
      "3. Split Strategy:\n",
      "   - Type: PurgedGroupTimeSeriesSplit\n",
      "   - test_ratio: 0.2\n",
      "   - n_splits: 5\n",
      "   - max_train_group_size: 400\n",
      "   - max_test_group_size: 200\n",
      "   - group_gap: 50\n",
      "\n",
      "4. Save Location:\n",
      "   - Path: ./models/pipeline.pkl\n",
      "   - Dataset Name: alvinlee9/jane-street-model-v4\n",
      "   - Timestamp: 2024-11-26 01:04:57\n",
      "\n",
      "Pipeline saved successfully! ✓\n",
      "\n",
      "Loading train data from partitions: [8, 9]\n",
      "Loaded train data shape: (12414600, 92)\n",
      "Date range in loaded data: 1360 to 1698\n",
      "Memory usage of dataframe is 4212.19 MB\n",
      "Memory usage after optimization is: 4212.19 MB\n",
      "Decreased by 0.0%\n",
      "\n",
      "Uploading pipeline to Kaggle...\n",
      "\n",
      "==================================================\n",
      "Pipeline Information:\n",
      "==================================================\n",
      "\n",
      "1. Model Configuration:\n",
      "   - Model Type: neural_network\n",
      "   - Number of Features: 85\n",
      "   - Model Parameters:\n",
      "     * hidden_dims: [256, 128, 64]\n",
      "     * dropout: 0.1\n",
      "     * learning_rate: 0.001\n",
      "     * weight_decay: 1e-05\n",
      "     * batch_size: 1024\n",
      "     * epochs: 100\n",
      "     * patience: 10\n",
      "\n",
      "2. Data Processing:\n",
      "   - Preprocessor: default_preprocessor\n",
      "     * Version: 1.0.0\n",
      "     * Description: Initial preprocessor with basic cleaning\n",
      "   - Feature Generator: default_feature_generator\n",
      "     * Version: 1.1.0\n",
      "     * Description: Added time-based features and symbol_id processing\n",
      "   - First 5 Features: ['feature_symbol_id', 'feature_sin_time_id', 'feature_cos_time_id', 'feature_sin_time_id_halfday', 'feature_cos_time_id_halfday']\n",
      "\n",
      "3. Split Strategy:\n",
      "   - Type: PurgedGroupTimeSeriesSplit\n",
      "   - test_ratio: 0.2\n",
      "   - n_splits: 5\n",
      "   - max_train_group_size: 400\n",
      "   - max_test_group_size: 200\n",
      "   - group_gap: 50\n",
      "\n",
      "4. Save Location:\n",
      "   - Path: ./models/jane-street-model-v4.pkl\n",
      "   - Dataset Name: alvinlee9/jane-street-model-v4\n",
      "   - Timestamp: 2024-11-26 01:04:59\n",
      "\n",
      "Pipeline saved successfully! ✓\n",
      "Creating new dataset: alvinlee9/jane-street-model-v4\n",
      "Starting upload for file jane-street-model-v4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282k/282k [00:01<00:00, 237kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: jane-street-model-v4.pkl (282KB)\n",
      "Dataset created successfully\n",
      "\n",
      "Evaluating on holdout test set...\n",
      "Starting prediction...\n",
      "Available features: ['feature_symbol_id', 'feature_sin_time_id', 'feature_cos_time_id', 'feature_sin_time_id_halfday', 'feature_cos_time_id_halfday', 'feature_weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\n",
      "X shape: (12414600, 85)\n",
      "X shape: (12414600, 85)\n",
      "X type: <class 'numpy.ndarray'>\n",
      "After conversion - X shape: (12414600, 85)\n",
      "After conversion - X type: <class 'numpy.ndarray'>\n",
      "After conversion - X dtype: float32\n",
      "Predictions shape: (12414600,)\n",
      "Feature names: ['feature_symbol_id', 'feature_sin_time_id', 'feature_cos_time_id', 'feature_sin_time_id_halfday', 'feature_cos_time_id_halfday', 'feature_weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\n",
      "First row of features: [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  1.86562097e+00  2.89147305e+00  3.53331715e-01\n",
      "  1.66805303e+00  2.85353088e+00  3.41232014e+00 -1.24521911e+00\n",
      "  1.17988408e+00  7.09262967e-01  7.84448981e-01  1.10000000e+01\n",
      "  7.00000000e+00  7.60000000e+01 -7.11436570e-01  2.20521331e+00\n",
      "  2.42864266e-02 -1.00000000e+00 -1.85819983e-01 -1.00000000e+00\n",
      " -7.76698649e-01 -1.42715228e+00 -1.06672890e-01 -1.89060077e-01\n",
      "  1.17183828e+00  7.74659991e-01  4.94448990e-02  4.24965657e-02\n",
      "  8.59838486e-01  6.86104715e-01  1.07405953e-01 -4.46071178e-01\n",
      " -3.96081984e-01 -1.90674901e-01 -1.00000000e+00 -1.00000000e+00\n",
      "  2.52633071e+00  3.01820421e+00  2.03940916e+00  1.31427348e-01\n",
      "  1.38990059e-01 -1.00000000e+00  1.41896889e-01 -1.00000000e+00\n",
      " -1.00000000e+00 -3.73337299e-01 -1.00000000e+00 -2.06047106e+00\n",
      "  1.22411931e+00  1.84256375e-01  2.77503520e-01  1.03839982e+00\n",
      " -1.00000000e+00  2.80564070e-01 -1.00000000e+00 -1.00000000e+00\n",
      " -1.22972775e+00 -1.00000000e+00 -2.17914605e+00  1.30260658e+00\n",
      " -1.00000000e+00 -1.03214657e+00 -6.31189704e-01 -2.03852344e+00\n",
      " -1.89215049e-01 -2.20135882e-01 -3.92400295e-01 -1.13548636e+00\n",
      " -1.66163313e+00 -7.90605903e-01  1.89021492e+00  1.34866301e-03\n",
      " -1.05867851e+00  2.24746418e+00  7.35395476e-02 -1.00000000e+00\n",
      " -1.00000000e+00 -3.96295749e-02 -8.48742351e-02 -2.79023081e-01\n",
      " -3.52221757e-01]\n",
      "Feature matrix shape: (12414600, 85)\n",
      "Holdout test R2 score: -3.1874\n",
      "\n",
      "Predicting on competition test set...\n",
      "Memory usage of dataframe is 0.01 MB\n",
      "Memory usage after optimization is: 0.01 MB\n",
      "Decreased by 6.0%\n",
      "Test data shape: (39, 85)\n",
      "Starting prediction...\n",
      "Available features: ['feature_symbol_id', 'feature_sin_time_id', 'feature_cos_time_id', 'feature_sin_time_id_halfday', 'feature_cos_time_id_halfday', 'feature_weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\n",
      "X shape: (39, 85)\n",
      "X shape: (39, 85)\n",
      "X type: <class 'polars.dataframe.frame.DataFrame'>\n",
      "After conversion - X shape: (39, 85)\n",
      "After conversion - X type: <class 'numpy.ndarray'>\n",
      "After conversion - X dtype: float32\n",
      "Predictions shape: (39,)\n",
      "Feature names: ['feature_symbol_id', 'feature_sin_time_id', 'feature_cos_time_id', 'feature_sin_time_id_halfday', 'feature_cos_time_id_halfday', 'feature_weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\n",
      "First row of features: shape: (1, 85)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ feature_s ┆ feature_s ┆ feature_c ┆ feature_s ┆ … ┆ feature_7 ┆ feature_7 ┆ feature_7 ┆ feature_ │\n",
      "│ ymbol_id  ┆ in_time_i ┆ os_time_i ┆ in_time_i ┆   ┆ 5         ┆ 6         ┆ 7         ┆ 78       │\n",
      "│ ---       ┆ d         ┆ d         ┆ d_halfday ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ i8        ┆ ---       ┆ ---       ┆ ---       ┆   ┆ f32       ┆ f32       ┆ f32       ┆ f32      │\n",
      "│           ┆ f64       ┆ f64       ┆ f64       ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0         ┆ 0.0       ┆ 1.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ -0.0      ┆ -0.0     │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "Feature matrix shape: (39, 85)\n",
      "Test predictions shape: (39,)\n",
      "[ 0.03168527  0.00067979 -0.00589953 -0.0078891   0.00167188  0.00466003\n",
      "  0.02503282  0.00855732  0.02326649  0.03025813  0.02032671  0.01450954\n",
      "  0.00598773  0.00300061  0.01175049  0.01093529 -0.02620169 -0.02172278\n",
      " -0.00550432 -0.04551613 -0.02374741 -0.03374496 -0.04806787 -0.06150777\n",
      " -0.07695631 -0.09175147 -0.11091108 -0.12634324 -0.14265627 -0.1553098\n",
      " -0.17371708 -0.17527421 -0.18491206 -0.19475657 -0.2586818  -0.218133\n",
      " -0.37227675 -0.24753389 -0.3372809 ]\n"
     ]
    }
   ],
   "source": [
    "INFERENCE_ONLY = False  # True: inference만 실행, False: 학습 포함\n",
    "OPTIMIZE_HYPERPARAMS = False  # True: 하이퍼파라미터 최적화 실행\n",
    "NICKNAME = \"alvinlee9\"  # Kaggle nickname\n",
    "BASE_DATASET_NAME = \"jane-street-model-v4\"  # Base dataset name\n",
    "DATASET_NAME = f\"{BASE_DATASET_NAME}\" if IS_KAGGLE else f\"{NICKNAME}/{BASE_DATASET_NAME}\"\n",
    "\n",
    "if INFERENCE_ONLY and IS_KAGGLE:\n",
    "    # Inference only mode\n",
    "    print(\"Running in inference-only mode...\")\n",
    "    pipeline = run_inference_only(DATASET_NAME)\n",
    "else:\n",
    "    # Training mode\n",
    "    # config = Config(\n",
    "    #     # partition_range=[6,7,8,9],\n",
    "    #     model=ModelConfig(\n",
    "    #         name='lightgbm',\n",
    "    #         params={\n",
    "    #             'objective': 'regression_l2',\n",
    "    #             'metric': 'rmse',\n",
    "    #             'boosting_type': 'gbdt',\n",
    "    #             'learning_rate': 0.1,\n",
    "    #             'random_state': 42,\n",
    "    #             'verbose': 1,\n",
    "    #             'device': 'cpu',\n",
    "    #         },\n",
    "    #         custom_metrics={},\n",
    "    #     ),\n",
    "    #     dataset_name=DATASET_NAME,\n",
    "    #     split_strategy=PurgedGroupTimeSeriesSplit(W\n",
    "    #         # n_splits=5,\n",
    "    #         # group_gap=15,\n",
    "    #         # max_train_group_size=200,\n",
    "    #         # max_test_group_size=50,\n",
    "    #         # test_ratio=0.2,\n",
    "    #         n_splits=5,\n",
    "    #         group_gap=50,\n",
    "    #         max_train_group_size=400,\n",
    "    #         max_test_group_size=200,\n",
    "    #         test_ratio=0.2,\n",
    "    #     ),\n",
    "    #     seed=42\n",
    "    # )\n",
    "    # config = Config(\n",
    "    #     # partition_range=[6,7,8,9],\n",
    "    #     model=ModelConfig(\n",
    "    #         name='xgboost',\n",
    "    #         params={\n",
    "    #             'objective': 'reg:squarederror',  # regression task\n",
    "    #             'eval_metric': 'rmse',            # evaluation metric\n",
    "    #             'booster': 'gbtree',             # use tree booster\n",
    "    #             'learning_rate': 0.1,            # eta\n",
    "    #             'max_depth': 6,                  # maximum tree depth\n",
    "    #             'min_child_weight': 1,           # minimum sum of instance weight in a child\n",
    "    #             'subsample': 0.8,                # sampling ratio of training instances\n",
    "    #             'colsample_bytree': 0.8,         # sampling ratio of columns when constructing each tree\n",
    "    #             'colsample_bylevel': 0.8,        # sampling ratio of columns for each level\n",
    "    #             'lambda': 1,                     # L2 regularization\n",
    "    #             'alpha': 0,                      # L1 regularization\n",
    "    #             'tree_method': 'hist',           # use histogram-based algorithm\n",
    "    #             'random_state': 42,\n",
    "    #             'n_jobs': -1,                    # use all CPU cores\n",
    "    #             'verbosity': 1,\n",
    "    #         },\n",
    "    #         custom_metrics={},\n",
    "    #     ),\n",
    "    #     dataset_name=DATASET_NAME,\n",
    "    #     split_strategy=PurgedGroupTimeSeriesSplit(\n",
    "    #         n_splits=5,\n",
    "    #         group_gap=50,\n",
    "    #         max_train_group_size=400,\n",
    "    #         max_test_group_size=200,\n",
    "    #         test_ratio=0.2,\n",
    "    #     ),\n",
    "    #     seed=42\n",
    "    # )\n",
    "    config = Config(\n",
    "        model=ModelConfig(\n",
    "            name='neural_network',\n",
    "            params={\n",
    "                'hidden_dims': [256, 128, 64],  # Hidden layer dimensions\n",
    "                'dropout': 0.1,                 # Dropout rate\n",
    "                'learning_rate': 1e-3,          # Learning rate\n",
    "                'weight_decay': 1e-5,           # L2 regularization\n",
    "                'batch_size': 1024,             # Batch size\n",
    "                'epochs': 100,                  # Maximum number of epochs\n",
    "                'patience': 10,                 # Early stopping patience\n",
    "            },\n",
    "            custom_metrics={},\n",
    "        ),\n",
    "        dataset_name=DATASET_NAME,\n",
    "        split_strategy=PurgedGroupTimeSeriesSplit(\n",
    "            n_splits=5,\n",
    "            group_gap=50,\n",
    "            max_train_group_size=400,\n",
    "            max_test_group_size=200,\n",
    "            test_ratio=0.2,\n",
    "        ),\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(config)\n",
    "    \n",
    "    if not IS_KAGGLE:\n",
    "        # Local training\n",
    "        print(\"Training model locally...\")\n",
    "        holdout_test = pipeline.train(\n",
    "            preprocessor=default_preprocessor,\n",
    "            feature_generator=default_feature_generator,\n",
    "            optimize=OPTIMIZE_HYPERPARAMS,\n",
    "            n_trials=100 if OPTIMIZE_HYPERPARAMS else None\n",
    "        )\n",
    "\n",
    "        print(\"\\nUploading pipeline to Kaggle...\")\n",
    "        pipeline.upload_to_kaggle()\n",
    "\n",
    "        # Evaluate on holdout test set using R2\n",
    "        print(\"\\nEvaluating on holdout test set...\")\n",
    "        holdout_test_X, holdout_test_y, holdout_test_w = pipeline.data_handler.get_feature_data(holdout_test)\n",
    "        holdout_test_pred = pipeline.predict(holdout_test_X)\n",
    "        \n",
    "        # Calculate R2 score\n",
    "        _, r2_score, _ = r2_metric(holdout_test_y, holdout_test_pred, holdout_test_w)\n",
    "        print(f\"Holdout test R2 score: {r2_score:.4f}\")\n",
    "        \n",
    "        # Predict on competition test set if available\n",
    "        if pipeline.data_handler.test_data is not None:\n",
    "            print(\"\\nPredicting on competition test set...\")\n",
    "            raw_test_data = pipeline.data_handler.test_data\n",
    "            test_data = pipeline.data_handler._process_and_generate_features(raw_test_data)\n",
    "            print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "            test_pred = pipeline.predict(test_data)\n",
    "            print(f\"Test predictions shape: {test_pred.shape}\")\n",
    "            print(test_pred)\n",
    "        \n",
    "        # Submit to competition\n",
    "        \n",
    "    else:\n",
    "        # Kaggle training\n",
    "        print(\"Training model in Kaggle environment...\")\n",
    "        pipeline.train(\n",
    "            preprocessor=default_preprocessor,\n",
    "            feature_generator=default_feature_generator,\n",
    "            optimize=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_KAGGLE:\n",
    "    import kaggle_evaluation.jane_street_inference_server\n",
    "\n",
    "    if not 'pipeline' in globals():  # pipeline이 아직 정의되지 않은 경우\n",
    "        # Inference only mode로 가정하고 모델 로드\n",
    "        pipeline = run_inference_only(DATASET_NAME)\n",
    "    \n",
    "    print(\"Setting up for competition submission...\")\n",
    "    inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(\n",
    "        predict\n",
    "    )\n",
    "    \n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        print(\"Starting inference server...\")\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        print(\"Running local gateway...\")\n",
    "        inference_server.run_local_gateway(\n",
    "            (f'{BASE_PATH}/test.parquet', f'{BASE_PATH}/lags.parquet')\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Experiment Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Custom preprocessing and feature generation example\\ndef my_preprocessor(df: pl.DataFrame) -> pl.DataFrame:\\n    return df.with_columns([\\n        pl.col('weight').fill_null(pl.col('weight').mean()),\\n        pl.col('feature_00').clip(-3, 3),\\n        pl.col('feature_01').clip(-3, 3),\\n    ])\\n\\ndef my_feature_generator(df: pl.DataFrame) -> pl.DataFrame:\\n    return df.with_columns([\\n        # Moving statistics\\n        pl.col('feature_00').rolling_mean(window_size=10).alias('feature_00_ma10'),\\n        pl.col('feature_00').rolling_std(window_size=10).alias('feature_00_std10'),\\n        \\n        # Feature interactions\\n        (pl.col('feature_02') / (pl.col('feature_03') + 1e-7)).alias('feature_ratio_02_03'),\\n        \\n        # Group statistics\\n        pl.col('feature_00').mean().over('symbol_id').alias('feature_00_symbol_mean'),\\n    ])\\n\\n# Run custom experiment\\nconfig = Config(...)\\npipeline = Pipeline(config)\\npipeline.train(\\n    preprocessor=my_preprocessor,\\n    feature_generator=my_feature_generator,\\n    optimize=True\\n)\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Custom preprocessing and feature generation example\n",
    "def my_preprocessor(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        pl.col('weight').fill_null(pl.col('weight').mean()),\n",
    "        pl.col('feature_00').clip(-3, 3),\n",
    "        pl.col('feature_01').clip(-3, 3),\n",
    "    ])\n",
    "\n",
    "def my_feature_generator(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        # Moving statistics\n",
    "        pl.col('feature_00').rolling_mean(window_size=10).alias('feature_00_ma10'),\n",
    "        pl.col('feature_00').rolling_std(window_size=10).alias('feature_00_std10'),\n",
    "        \n",
    "        # Feature interactions\n",
    "        (pl.col('feature_02') / (pl.col('feature_03') + 1e-7)).alias('feature_ratio_02_03'),\n",
    "        \n",
    "        # Group statistics\n",
    "        pl.col('feature_00').mean().over('symbol_id').alias('feature_00_symbol_mean'),\n",
    "    ])\n",
    "\n",
    "# Run custom experiment\n",
    "config = Config(...)\n",
    "pipeline = Pipeline(config)\n",
    "pipeline.train(\n",
    "    preprocessor=my_preprocessor,\n",
    "    feature_generator=my_feature_generator,\n",
    "    optimize=True\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
